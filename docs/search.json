[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "blog",
    "section": "",
    "text": "Blind date with a data talk\n\n\n\n\n\nPick a mood, watch a talk!\n\n\n\n\n\nMar 7, 2024\n\n\nIsabel Zimmerman\n\n\n\n\n\n\n\n\n\n\n\n\nWe’re making a blog (IDE agnostic edition)\n\n\n\n\n\nMaking a Quarto blog and deploying it on GitHub Pages, step by step.\n\n\n\n\n\nSep 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nA year in review: vetiver\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2022\n\n\nIsabel Zimmerman\n\n\n\n\n\n\n\n\n\n\n\n\nPyenv in RStudio\n\n\n\n\n\nUsing pyenv in the RStudio IDE.\n\n\n\n\n\nSep 19, 2022\n\n\nIsabel Zimmerman\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple models on one API\n\n\n\n\n\nDeploying multiple models to one VetiverAPI.\n\n\n\n\n\nSep 10, 2022\n\n\nIsabel Zimmerman\n\n\n\n\n\n\n\n\n\n\n\n\nGithub Actions all the way down\n\n\n\n\n\nUsing Github Actions to create new repos that also run their own Github Actions.\n\n\n\n\n\nMar 30, 2022\n\n\nIsabel Zimmerman\n\n\n\n\n\n\n\n\n\n\n\n\nCat detection with explainers\n\n\n\n\n\nAnchor explainers on images to better understand model decision-making.\n\n\n\n\n\nMar 20, 2022\n\n\nisabel zimmerman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/pydatanyc2022/index.html",
    "href": "talks/pydatanyc2022/index.html",
    "title": "Holistic MLOps for better science",
    "section": "",
    "text": "Machine learning operations (MLOps) are often synonymous with large and complex applications, but many MLOps practices help practitioners build better models, regardless of the size. This talk shares best practices for operationalizing a model and practical examples using the open-source MLOps framework vetiver to version, share, deploy, and monitor models.\nView the repository || Watch the recording\n\n\n\nPyData always hosts such a great conference, and PyData NYC 2022 was no exception! I met so many cool people, learned a lot of lore about pandas from other talks, and ate some delicious NY-style pizza. This talk is a good reference for explaining why the vetiver framework emphasizes pinning a model and reading it into a Dockerfile, rather than putting the model binary into the Docker container. In short, this is due to the difficulty of updating a model later and doing ad hoc analysis."
  },
  {
    "objectID": "talks/pydatanyc2022/index.html#abstract",
    "href": "talks/pydatanyc2022/index.html#abstract",
    "title": "Holistic MLOps for better science",
    "section": "",
    "text": "Machine learning operations (MLOps) are often synonymous with large and complex applications, but many MLOps practices help practitioners build better models, regardless of the size. This talk shares best practices for operationalizing a model and practical examples using the open-source MLOps framework vetiver to version, share, deploy, and monitor models.\nView the repository || Watch the recording\n\n\n\nPyData always hosts such a great conference, and PyData NYC 2022 was no exception! I met so many cool people, learned a lot of lore about pandas from other talks, and ate some delicious NY-style pizza. This talk is a good reference for explaining why the vetiver framework emphasizes pinning a model and reading it into a Dockerfile, rather than putting the model binary into the Docker container. In short, this is due to the difficulty of updating a model later and doing ad hoc analysis."
  },
  {
    "objectID": "talks/berlinbuzzwords2021/index.html",
    "href": "talks/berlinbuzzwords2021/index.html",
    "title": "Explaining model explainability",
    "section": "",
    "text": "Machine learning doesn’t have the same objectives as its users. While models look to optimize a function using the given data, humans look to gain insight into their problems. At best, these two objectives align; at worst, machine learning models make the front page of the news for unintended, but astonishing bias. Model explainability algorithms allow data scientists to understand not only what the model outcome is, but why it is being made. This talk will explain what model explainability is, who should care, and show participants how/when to use multiple types of explainability algorithms.\nThis session shows the usefulness of a variety of algorithms, but also discusses the limitations. Told from a data scientist’s point of view, this session provides a use case scenario exposing unintended bias using healthcare data. The audience will learn: the basics of model explainability, why this is a relevant issue, how model explainability offers insight into unintended bias, and know how to deploy explainability algorithms in Python with alibi, the open-source library from Seldon.\nWatch the recording"
  },
  {
    "objectID": "talks/berlinbuzzwords2021/index.html#abstract",
    "href": "talks/berlinbuzzwords2021/index.html#abstract",
    "title": "Explaining model explainability",
    "section": "",
    "text": "Machine learning doesn’t have the same objectives as its users. While models look to optimize a function using the given data, humans look to gain insight into their problems. At best, these two objectives align; at worst, machine learning models make the front page of the news for unintended, but astonishing bias. Model explainability algorithms allow data scientists to understand not only what the model outcome is, but why it is being made. This talk will explain what model explainability is, who should care, and show participants how/when to use multiple types of explainability algorithms.\nThis session shows the usefulness of a variety of algorithms, but also discusses the limitations. Told from a data scientist’s point of view, this session provides a use case scenario exposing unintended bias using healthcare data. The audience will learn: the basics of model explainability, why this is a relevant issue, how model explainability offers insight into unintended bias, and know how to deploy explainability algorithms in Python with alibi, the open-source library from Seldon.\nWatch the recording"
  },
  {
    "objectID": "talks/positconf2023/index.html",
    "href": "talks/positconf2023/index.html",
    "title": "Thanks, I made it in quartodoc",
    "section": "",
    "text": "When Python package developers create documentation, they typically must choose between mostly auto-generated docs or writing all the docs by hand. This is problematic since effective documentation has a mix of function references, high-level context, examples, and other content. Quartodoc is a new documentation system that automatically generates Python function references within Quarto websites. This talk will discuss pkgdown’s success in the R ecosystem and how those wins can be replicated in Python with quartodoc examples. Listeners will walk away knowing more about what makes documentation delightful (or painful), when to use quartodoc, and how to use this tool to make docs for a Python package.\nView the repository\n\n\n\nI had so much fun as posit::conf(2023)! The community here is ELECTRIC. Everyone is so engaged and interested and genuine (and nerdy). It feels like a show and tell with all your best friends. I had the pleasure of talking about my documentation journey with Python packages, and how the project quartodoc felt just right."
  },
  {
    "objectID": "talks/positconf2023/index.html#abstract",
    "href": "talks/positconf2023/index.html#abstract",
    "title": "Thanks, I made it in quartodoc",
    "section": "",
    "text": "When Python package developers create documentation, they typically must choose between mostly auto-generated docs or writing all the docs by hand. This is problematic since effective documentation has a mix of function references, high-level context, examples, and other content. Quartodoc is a new documentation system that automatically generates Python function references within Quarto websites. This talk will discuss pkgdown’s success in the R ecosystem and how those wins can be replicated in Python with quartodoc examples. Listeners will walk away knowing more about what makes documentation delightful (or painful), when to use quartodoc, and how to use this tool to make docs for a Python package.\nView the repository\n\n\n\nI had so much fun as posit::conf(2023)! The community here is ELECTRIC. Everyone is so engaged and interested and genuine (and nerdy). It feels like a show and tell with all your best friends. I had the pleasure of talking about my documentation journey with Python packages, and how the project quartodoc felt just right."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "isabel zimmerman",
    "section": "",
    "text": "Here’s a non-exhaustive list of talks I have given. I try to highlight talks that I have slides and a recording available to share. Oftentimes my talks include silly doodles to illustrate technical points, which are most of the talk images shown here. This started from partially an irrational fear of accidentally miscrediting content and partially due to the fact it’s really hard to find an image that is just right. Nowadays, the doodles are just a whimsical preparatory step that I enjoy when writing talks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThanks, I made it in quartodoc\n\n\n\n\n\nPresented at posit::conf(2023)\n\n\n\n\n\nSep 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPractical MLOps for better models\n\n\n\n\n\nPresented at PyData Global 2022\n\n\n\n\n\nDec 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHolistic MLOps for better science\n\n\n\n\n\nPresented at PyData NYC 2022\n\n\n\n\n\nNov 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding an MLOps strategy from the ground up\n\n\n\n\n\nPresented at Crunch 2022\n\n\n\n\n\nOct 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nDemystifying MLOps\n\n\n\n\n\nPresented at rstudio::conf(2022)\n\n\n\n\n\nJul 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nExplaining model explainability\n\n\n\n\n\nPresented at Berlin Buzzwords 2021\n\n\n\n\n\nOct 3, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/pyenv-in-rstudio/pyenv-crying.html",
    "href": "posts/pyenv-in-rstudio/pyenv-crying.html",
    "title": "Pyenv in RStudio",
    "section": "",
    "text": "I am a chronic destroyer of Python environments. My favorite tools right now are pyenv and pyenv-virtualenv, which keep me mostly out of trouble, but there is some extra legwork to get them set up with RStudio IDE.\nI usually start by checking what Python is being used by going to the RStudio console and running:\nreticulate::py_config()\nThis gives me:\npython:         /usr/bin/python3\nlibpython:      /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/config-3.8-darwin/libpython3.8.dylib\npythonhome:     /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8:/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8\nversion:        3.8.9 (default, Aug  3 2021, 19:21:54)  [Clang 13.0.0 (clang-1300.0.29.3)]\nnumpy:           [NOT FOUND]\nwhich is nice, since at least some sort of Python is THERE, but I want to follow the Golden Rule: don’t touch the system Python.\nTime to do some environment gymnastics. My first move is to allow pyenv to share its environment, which is required to use envs with reticulate.\nenv PYTHON_CONFIGURE_OPTS=\"--enable-shared\" pyenv install 3.9.4\npyenv virtualenv 3.9.4 newenv\n\n\n\n\n\n\nNote\n\n\n\nIf you skip this step, you will get an error similar to:\nError: '/Users/isabelzimmerman/.pyenv/versions/3.9.11/envs/pydemo/bin/python' was not \nbuilt with a shared library. reticulate can only bind to copies of Python \nbuilt with '--enable-shared'.\n\n\nNext, we’re going to restart the R session, and try to just add this new Python environment in there. The key here is RESTART R SESSION (on macOS, this is commmand+shift+0) and then DON’T TRY TO RUN ANYTHING IN PYTHON BEFORE THESE NEXT FEW COMMANDS. If you “just try to run one thing to see if {insert ridiculous Python MultiEnvironmentVerse of Madness strategy} worked,” you will end up in an endless loop of setting environment variables that are ignored since they are already initialized (this sentence is hard-won for me). Let’s try to use reticulate to set Python first.\nreticulate::use_python('/Users/isabelzimmerman/.pyenv/versions/3.9.4/envs/newenv/bin/python', required = TRUE)\ngives us:\nWarning message:\nThe request to `use_python(\"/Users/isabelzimmerman/.pyenv/versions/3.9.11/envs/newenv/bin/python\")` \nwill be ignored because the environment variable RETICULATE_PYTHON is set to \"/usr/bin/python3\" \nThis is a helpful warning that tells us we need to set up the RETICULATE_PYTHON environment variable. We can do this by:\nSys.setenv(RETICULATE_PYTHON='/Users/isabelzimmerman/.pyenv/versions/3.9.4/envs/newenv/bin/python')\nand then check to see if it worked since we have pretty strong trust issues.\nSys.getenv(\"RETICULATE_PYTHON\")\n[1] \"/Users/isabelzimmerman/.pyenv/versions/3.9.4/envs/newenv/bin/python\"\nPromising! Let’s check out the reticulate::py_config() now.\nreticulate::py_config()\npython:         /Users/isabelzimmerman/.pyenv/versions/3.9.4/envs/newenv/bin/python\nlibpython:      /Users/isabelzimmerman/.pyenv/versions/3.9.4/lib/libpython3.9.dylib\npythonhome:     /Users/isabelzimmerman/.pyenv/versions/3.9.4/envs/newenv:/Users/isabelzimmerman/.pyenv/versions/3.9.4/envs/newenv\nversion:        3.9.4 (default, May 31 2022, 09:32:34)  [Clang 13.0.0 (clang-1300.0.29.3)]\nnumpy:          /Users/isabelzimmerman/.pyenv/versions/3.9.4/envs/newenv/lib/python3.9/site-packages/numpy\nnumpy_version:  1.23.1\n\nNOTE: Python version was forced by RETICULATE_PYTHON\n\n\n\n\n\n\nNote\n\n\n\nIf this doesn’t work, I would tell you to go digging for your RETICULATE_PYTHON environment variable. Some places to check out:\nvi ~/.Renviron\nshould open a file that has environment variables, specifically our elusive RETICULATE_PYTHON. We can update that file to look like:\nexport RETICULATE_PYTHON=/Users/isabelzimmerman/.pyenv/versions/3.9.4/envs/newenv/bin/python\n\n\nIT WORKED! Time to run all the Python I can. I’m fully open to better workflows if you have them, but for now, this gets me on my way when I feel the despair of another broken Python environment.\nAlso, S/O to Firas Sadiyah’s blog post on pyenv + RStudio which has helped me many a time to get on the right track!"
  },
  {
    "objectID": "posts/blind-date-with-a-data-talk/index.html",
    "href": "posts/blind-date-with-a-data-talk/index.html",
    "title": "Blind date with a data talk",
    "section": "",
    "text": "A “blind date with a book” is where a book gets wrapped up in nondescript paper, and all the intro you get is a vague bullet point or two. You can’t judge a book by it’s cover, since, well, you can’t see it. You have to trust the synopsis and enjoy the adventure.\nI got the idea to make a “blind date” with a data science talk after starting to curate a list of my favorite talks for a friend. Unrelated, I have been itching to play around with the shinylive project to put some interactive things on my blog. Why not put these two together?\n#| standalone: true\n#| viewerHeight: 300\n\nimport random\nfrom shiny import reactive, render\nfrom shiny.express import input, ui\nfrom pathlib import Path\n\ncss_file = Path(__file__).parent / \"styles.css\"\n\ndates = {\n    \"humor\": [\n        {\n            \"name\": \"Joel Grus: I don't like notebooks\",\n            \"link\": \"https://www.youtube.com/watch?v=7jiPeIFXb6U\"\n        }, # joel grus i dont like notebooks\n        {\n            \"link\":\"https://www.youtube.com/watch?v=hfqjyeA_z7s&list=PL9HYL-VRX0oRFZslRGHwHuwea7SvAATHp&index=40\",\n            \"name\": \"Hadley Wickham: It's a Great Time to be an R Package Developer!\"}, # hadley slide karaoke\n        {\n            \"link\":\"https://www.youtube.com/watch?v=C9OlkY87vf8&list=UULFYhY67dMuJ_w_9_cfkaruWQ&index=8\",\n            \"name\": \"Vicki Boykis: An ML Fairytale\"\n        }, # vicki boykis ml fairy tale\n        {\n            \"link\": \"https://www.youtube.com/watch?v=D48NQTNg19s&list=UULFYhY67dMuJ_w_9_cfkaruWQ&index=40\",\n            \"name\":\"Jacqueline Nolis: Alaska challenged my preconceived notions of storing sunset data\"\n        }, # jacqueline nolis time\n        {\n            \"link\": \"https://www.youtube.com/watch?v=Pm9C-Cz4bXE&t\",\n            \"name\": \"JD Long: I'd have written a shorter solution but I didn't have the time\"\n        } # jd long shorter \n        ],\n    \"story\": [\n        {\n            \"link\": \"https://youtu.be/Pa1PNfoOp-I?si=rF3udat0nV9n04w6\",\n            \"name\": \"JD Long: It's Abstractions All the Way Down...\"\n        }, # jd long abstractions\n        {\n            \"link\": \"https://youtu.be/DVQJ39_9L0U?si=PbsjmQR96rrz7scy\",\n            \"name\": \"Kari Jordan: Black Hair and Data Science Have More in Common Than You Think\"\n        }, # kari jordan black hair\n        {\n            \"link\": \"https://www.youtube.com/watch?v=g1ib43q3uXQ\",\n            \"name\": \"Felienne Hermans: How to teach programming (and other things)?\"\n        }, #  felienne hermans excel\n        {\n            \"link\": \"https://www.youtube.com/watch?v=s3WbEfoxRjs\",\n            \"name\": \"Sophie Watson: What they didn’t teach you in Grad School\"}, # sophie watson what they didnt teach u\n        ],\n    \"nerd\": [\n        {\n            \"link\": \"https://www.youtube.com/watch?v=sYliwvml9Es\",\n            \"name\": \"Jeremy Howard: A hacker's guide to open source LLMs\"\n        }, # jeremy howard llm\n        {\n            \"link\": \"https://www.youtube.com/watch?v=-YEUFGFHWgQ\",\n            \"name\": \"Calvin Hendryx-Parker: Bootstrapping Your Local Python Environment\"}, # calvin hendryx-parker bootstrapping python\n        {\n            \"link\": \"https://www.youtube.com/watch?v=7nNB__jK9AY\",\n            \"name\": \"Alison Presmanes Hill: The Happiest Notebooks on Earth\"\n        }, # \n        {\n            \"link\": \"https://www.youtube.com/watch?v=EvQUVzDJRJ8&list=UULFYhY67dMuJ_w_9_cfkaruWQ&index=42\",\n            \"name\": \"Chelsea Parlett-Pelleriti: Why Are You The Way That You Are: Sklearn Quirks\"\n        } # chelsea parlett scikit\n        ],\n    \"reveal\": [\n        {\n            \"link\": \"https://www.youtube.com/watch?v=qKfkCY7cmBQ\",\n            \"name\": \"Peter Wang: Programming for everyone (or the next 100 million Pythonistas)\"}, # peter wang pyright\n        {\n            \"link\": \"https://www.youtube.com/watch?v=HpqLXB_TnpI\",\n            \"name\": \"Joe Cheng: The Past and Future of Shiny\"\n        }, # joe cheng shiny py \n        {\n            \"link\": \"https://www.youtube.com/watch?v=p7Hxu4coDl8\", \n            \"name\": \"Mine Çetinkaya-Rundel & Julia Stewart Lowndes: Hello Quarto: Share, Collaborate, Teach, Reimagine\"\n        }, # mine and julia hello quarto\n    ],\n    \"small\": [\n        {\n            \"link\": \"https://www.destroyallsoftware.com/talks/wat\",\n            \"name\": \"Destroy all software: WAT\"\n        }, # wat\n        {\n            \"link\": \"https://youtu.be/V3XdLVAwmX0?si=5gPV2bXsgURLqCYG\",\n            \"name\": \"Libby Heeren: Why You Should Stop Networking and Start Making Friends\"\n        }, # libby heeren stop networking\n        {\n            \"link\": \"https://youtu.be/ZDK5DZOgHD8?si=3q1q0Zo0z-Oet_Bb&t=1150\",\n            \"name\": \"Katy Huff: I do (automate things)\"\n        }, # katy huff wedding website\n        {\n            \"link\": \"https://www.youtube.com/watch?v=ES1LTlnpLMk&list=UULFYhY67dMuJ_w_9_cfkaruWQ&index=41\",\n            \"name\": \"Jenny Bryan: How to name files\"\n        } # jenny bryan file name\n        ]\n}\n\nui.include_css(css_file)\n\nui.input_radio_buttons(  \n    \"radio\",  \n    \"I want a data science talk with:\",  \n    {\n        \"humor\": \"Lots of humor\", \n        \"story\": \"A good story\", \n        \"nerd\": \"Maximum nerdiness\",\n        \"reveal\": \"An exciting new tech for the time\",\n        \"small\": \"Something bite-sized\"\n    },  \n)\nui.input_action_button(\"action_button\", \"Find me a talk 🔮\")  \n\n@render.express()\n@reactive.event(input.action_button)\ndef _():\n    k = input.radio()\n    rec_talk = random.choice(dates[k])\n    ui.HTML(f'&lt;p&gt;&lt;/p&gt;&lt;a href={rec_talk[\"link\"]} target=\"_blank\" color=\"purple\"&gt;{rec_talk[\"name\"]}&lt;/a&gt;')\n\n## file: styles.css\nbody {\n    background-color: #FDFCFC;\n    color: #404040;\n  }\na {\n  color: #404040;\n}\nShoutout to Madison Yonash for asking about new data science talks and effectively nerd sniping me into spending an evening building this."
  },
  {
    "objectID": "posts/multiple-models-api/models.html",
    "href": "posts/multiple-models-api/models.html",
    "title": "Multiple models on one API",
    "section": "",
    "text": "As someone who identifies as a Chicago Bears fan for life, I have some strong superstitions rituals on gameday. In a completely hypothetical example, I might have one model that I feel must be used when the Bears have a home game, and a different model for any other day. (They always win when I use this model to write blog posts on game days, and I fear a replay of the 2016 season when our win-loss ratio was 3-13 if I change my ways.)\nOftentimes you need to deploy more than one model. Even if the fate of your favorite NFL team isn’t depending upon your models’ locations, it’s important to understand where these models should be living. tldr;\n\nInput data is the same -&gt; use one API\nInput data is not the same -&gt; use multiple APIs\n\n(These are not definitive rules. A lot of this is dependent upon architecture, how your deployment is set up, what works best for your organization, etc. Also, vetiver allows you to break both of these rules!)\nIf your models are unrelated and the input data is different, you probably want to put them on different APIs. However, if the input data is the same for multiple different models, it might make sense to deploy them on the same API, but at different endpoints.\nFor our Chicago train ridership data, we ALWAYS want to predict ridership from the same parameters every time. However, if the data indicates it was a home game for the Chicago Bears, we want to use our lucky model.\nLet’s start by loading some data and use tidymodels to put all our preprocessing in one recipe.\n\nlibrary(tidymodels)\ndata(Chicago)\n\nchicago_small &lt;- Chicago %&gt;% slice(1:730) # two years of data\n\nchicago_rec &lt;-\n  recipe(ridership ~ ., data = Chicago) %&gt;%\n  step_date(date) %&gt;%\n  step_holiday(date, keep_original_cols = FALSE) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  step_pca(all_of(stations), num_comp = 4)\n\nWe can then use tidymodels to make multiple different models quickly. Notice that I am actually putting my preprocessing recipe in the recipe with my model. This is intentional, and best practice for modeling.\n\n\n\n\n\n\nImportant\n\n\n\nFeature engineering is part of your model workflow. You should be packaging this up with your model and deploying it as part of your model workflow. In Python, this involves using something like scikit learn Pipelines. In R, this involves using something like tidymodels workflows.\n\n\n\nBuild a few models for home games (support vectors for extra support at home):\n\ntree_model &lt;-\n  svm_linear() %&gt;%\n  set_engine(\"LiblineaR\") %&gt;%\n  set_mode(\"regression\")\n\nhome_game_model &lt;-\n  workflow(chicago_rec, tree_model) %&gt;%\n  fit(chicago_small)\n\nAnd not home games:\n\nlinear_model &lt;-\n  linear_reg() %&gt;%\n  set_engine(\"lm\")\n\nnot_home_game_model &lt;-\n  workflow(chicago_rec, tree_model) %&gt;% \n  fit(chicago_small)\n\nNext, I will make some deployable model objects, or vetiver_model, with the vetiver package.\n\nlibrary(vetiver)\n\nhome &lt;- vetiver_model(home_game_model, \"home\")\nnot_home &lt;- vetiver_model(not_home_game_model, \"not_home\")\n\nNormally we would be iterating through models and possibly versioning them as well, but let’s skip that step and go right to deployment.\nI’ll deploy these two models on ONE plumber API by using a combination of vetiver_pr_post() and vetiver_pr_docs() along with the path argument.\n\nlibrary(plumber)\n\npr() %&gt;% \n  vetiver_pr_post(home, path = \"/home_game\") %&gt;% \n  vetiver_pr_docs(home) %&gt;% \n  vetiver_pr_post(not_home, path = \"/not_home_game\") %&gt;% \n  vetiver_pr_docs(not_home) %&gt;% \n  pr_run()\n\nNow, to make predictions, I will route data to each endpoint respectively. To do this on your own computer, you will have to run the above commands as background job (or deploy it with docker.)\nWe can make predictions at each endpoint with data. Of course this can be as complex or simple as you desire, but here’s the meat-and-potatoes of it.\n\nhome_endpoint &lt;- vetiver_endpoint('http://127.0.0.1:5331/home_game')\n\nhome_data &lt;- Chicago %&gt;%\n  filter(Bears_Home == 1) %&gt;%\n  tail(5)\n\npredict(home_endpoint, home_data)\n\n\nnot_home_endpoint &lt;- vetiver_endpoint('http://127.0.0.1:5331/not_home_game')\n\nnot_home_data &lt;- Chicago %&gt;%\n  filter(Bears_Home == 0) %&gt;%\n  tail(5)\n\npredict(not_home_endpoint, not_home_data)\n\nWe have now created an API with multiple models at various endpoints, and successfully interacted with them! This is a great start to making more complex MLOps workflows with vetiver."
  },
  {
    "objectID": "posts/pet-image-detection/image-explainability.html",
    "href": "posts/pet-image-detection/image-explainability.html",
    "title": "Cat detection with explainers",
    "section": "",
    "text": "The growing availability of big data has increased the benefits of using complex models, forcing data scientists to choose between accuracy and interpretability of a model’s output. Adding explainability methods to models that are not easily understood helps:\n\nEnsure transparency algorithmic decision-making\nIdentify potential bias in the training data\nGive a deeper understanding of the process being modeled\n\nThese methods are able to give insight into why your model generates specific outputs; explainability algorithms are especially useful in highly regulated industries (ie, pinpoint the attributes that caused someone to be denied/approved a home loan). We’ll demonstrate an anchor explainer in this notebook to better understand why a generic image detection model is creating certain outputs using the open-source library alibi.\n\nimport os\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3, decode_predictions\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom alibi.explainers import AnchorImage\n\nTo begin, we load and format the photo into a machine readable array.\n\ndef image_pipeline(image_name):  \n\n    #Format custom picture\n    resized_image = load_img(image_name, target_size=(299, 299))\n    numpy_image = img_to_array(resized_image)\n    image_batch = np.expand_dims(numpy_image, axis=0)\n    \n    model = InceptionV3(weights='imagenet')\n    preds = model.predict(image_batch)\n    \n    #Display results\n    label = decode_predictions(preds, top=3)\n    plt.title(label[0])\n    plt.imshow(resized_image)\n\n\nimage_name = 'cat.jpg'\nimage_pipeline(image_name)\n\n\n\n\nThe predict function returns the predictions via Softmax, which mean that the prediction can be translated as the probability that the image falls into one of the categories given in training. Even though we see this as a cat, the model gives a 100% probability that the image shows a pitcher. There are a few things to note here.\n\nThis is a photo I took of my own cat, so I can confidently know that the model has never seen the picture before. To a human eye, it is (hopefully) pretty obvious that the image is of a cat. However, oftentimes training data, especially images, does not accurately reflect photos that are taken by you or I. For example, the training photos may all have centered images in good lighting with nothing in the background, which is unrealistic for everyday images.\nThis is a Softmax output. The model only gives us the names of its best guesses, not how strong the prediction is. This is a subtle differentiation. The model believes that pitcher is definitely the best guess, but it could only be 1% certain in this prediction. To solve this, we could add new outcome metrics, or tweak the model.\n\nBefore we do any of that though, it could be useful for us to better understand how the model is making this prediction. We’ll use something called an explainer to dive deeper."
  },
  {
    "objectID": "posts/pet-image-detection/image-explainability.html#pet-image-detection",
    "href": "posts/pet-image-detection/image-explainability.html#pet-image-detection",
    "title": "Cat detection with explainers",
    "section": "",
    "text": "The growing availability of big data has increased the benefits of using complex models, forcing data scientists to choose between accuracy and interpretability of a model’s output. Adding explainability methods to models that are not easily understood helps:\n\nEnsure transparency algorithmic decision-making\nIdentify potential bias in the training data\nGive a deeper understanding of the process being modeled\n\nThese methods are able to give insight into why your model generates specific outputs; explainability algorithms are especially useful in highly regulated industries (ie, pinpoint the attributes that caused someone to be denied/approved a home loan). We’ll demonstrate an anchor explainer in this notebook to better understand why a generic image detection model is creating certain outputs using the open-source library alibi.\n\nimport os\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3, decode_predictions\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom alibi.explainers import AnchorImage\n\nTo begin, we load and format the photo into a machine readable array.\n\ndef image_pipeline(image_name):  \n\n    #Format custom picture\n    resized_image = load_img(image_name, target_size=(299, 299))\n    numpy_image = img_to_array(resized_image)\n    image_batch = np.expand_dims(numpy_image, axis=0)\n    \n    model = InceptionV3(weights='imagenet')\n    preds = model.predict(image_batch)\n    \n    #Display results\n    label = decode_predictions(preds, top=3)\n    plt.title(label[0])\n    plt.imshow(resized_image)\n\n\nimage_name = 'cat.jpg'\nimage_pipeline(image_name)\n\n\n\n\nThe predict function returns the predictions via Softmax, which mean that the prediction can be translated as the probability that the image falls into one of the categories given in training. Even though we see this as a cat, the model gives a 100% probability that the image shows a pitcher. There are a few things to note here.\n\nThis is a photo I took of my own cat, so I can confidently know that the model has never seen the picture before. To a human eye, it is (hopefully) pretty obvious that the image is of a cat. However, oftentimes training data, especially images, does not accurately reflect photos that are taken by you or I. For example, the training photos may all have centered images in good lighting with nothing in the background, which is unrealistic for everyday images.\nThis is a Softmax output. The model only gives us the names of its best guesses, not how strong the prediction is. This is a subtle differentiation. The model believes that pitcher is definitely the best guess, but it could only be 1% certain in this prediction. To solve this, we could add new outcome metrics, or tweak the model.\n\nBefore we do any of that though, it could be useful for us to better understand how the model is making this prediction. We’ll use something called an explainer to dive deeper."
  },
  {
    "objectID": "posts/pet-image-detection/image-explainability.html#explainability",
    "href": "posts/pet-image-detection/image-explainability.html#explainability",
    "title": "Cat detection with explainers",
    "section": "Explainability",
    "text": "Explainability\nWe will use a local version of the model to build an anchor explainer. This will show us what parts of the photo the model used in order to give the “pitcher” prediction.\n\ndef explainability_pipeline(image_name): \n    \n    # format custom picture\n    resized_image = load_img(image_name, target_size=(299, 299))\n    numpy_image = img_to_array(resized_image)\n    image_batch = np.expand_dims(numpy_image, axis=0)\n    \n    # set hyperparameters\n    image_shape = (299, 299, 3)\n    segmentation_fn = 'slic'\n    kwargs = {'n_segments': 15, 'compactness': 20, 'sigma': .5}\n\n    # load model\n    model = InceptionV3(weights='imagenet')\n    \n    # explainer, from alibi library\n    explainer = AnchorImage(model, image_shape, segmentation_fn=segmentation_fn, \n                            segmentation_kwargs=kwargs, images_background=None)\n    \n    cat_image = image_batch[0]\n    explanation = explainer.explain(cat_image, threshold=.95, \\\n                                    p_sample=.5, tau=0.25)\n\n    plt.imshow(explanation.anchor)\n\n\nexplainability_pipeline(image_name)\n\nskimage.measure.label's indexing starts from 0. In future version it will start from 1. To disable this warning, explicitely set the `start_label` parameter to 1.\n\n\n\n\n\nIt looks as though the model uses primarily the background to create this prediction, so it comes as no surprise that the classification is wildly incorrect. With this information in mind, data scientists may decide to go back and create a more robust exploration of the data and model. It may be the case that the training data of cats has only solid backgrounds, different lighting, different color/hair length cat, or some other feature that caused this image to not be identified correctly.\nExplainers will not fix the model itself. However, they are useful tools for data scientists to build well-architected models by exposing bias in training data and giving transparency to black-box models."
  },
  {
    "objectID": "posts/pet-image-detection/image-explainability.html#try-your-own-pets",
    "href": "posts/pet-image-detection/image-explainability.html#try-your-own-pets",
    "title": "Cat detection with explainers",
    "section": "Try your own pets!",
    "text": "Try your own pets!\nEither download or git clone this demo repo to your local machine. Import your own photo to the data folder. Change your-image-name in the final cell (see below for example) to match your image’s name, and press run!\n\ncustom_image_name = '../data/your-image-name.jpg'\nimage_pipeline(custom_image_name)\nexplainability_pipeline(custom_image_name)"
  },
  {
    "objectID": "posts/state-of-vetiver/index.html",
    "href": "posts/state-of-vetiver/index.html",
    "title": "A year in review: vetiver",
    "section": "",
    "text": "It has been almost a year of vetiver! Vetiver for Python (0.1.8) and R (0.1.8) seeks to provide fluent tooling to version, share, deploy, and monitor a trained model."
  },
  {
    "objectID": "posts/state-of-vetiver/index.html#the-journey",
    "href": "posts/state-of-vetiver/index.html#the-journey",
    "title": "A year in review: vetiver",
    "section": "The journey",
    "text": "The journey\nRecently, the team dedicated a month to reading about the world of MLOps, as it is today. We all read the fantastic book Designing Machine Learning Systems by Chip Huyen and split up numerous other academic articles and web content between ourselves. The world of machine learning operations moves fast, and we wanted to ensure the choices we had made early on (eg, focusing on versioning, deploying, and monitoring) would serve data practitioners best.\nAfter reading many different definitions of MLOps, the one we found most useful is: “a set of practices to deploy and maintain machine learning models in production reliably and efficiently.” While not every MLOps practice is applicable at scale for every team, these best practices can elevate any size of project.\nMLOps applies the same basic principles as DevOps (development operations) in a specialized machine learning context. One common point of tension in MLOps is that data science is highly experimental and iterative compared to general purpose software delivery, but deploying to a production environment still requires reliable software engineering delivery practices. To help ease this pain point, APIs are commonly used to deploy models due to their stability and simplicity. APIs can be tested, and they act nearly identically in every architecture. This allows for software engineering practices to be applied to APIs. They have a straightforward architecture to configure and update, giving data scientists agility to retrain and update models as needed.\nTools labeled as “MLOps frameworks” have a broad scope. Tasks generally fall into one of a few different categories:\n\nOrchestration or pipelines\nExperiment tracking\nModel versioning\nModel deployment\nModel monitoring\n\nVetiver spans a few of these tasks, but is not a tool built for orchestration or experiment tracking. Rather, Vetiver focuses on the practices of versioning, deploying, and monitoring and will continue building support for these tasks."
  },
  {
    "objectID": "posts/state-of-vetiver/index.html#where-vetiver-is-now",
    "href": "posts/state-of-vetiver/index.html#where-vetiver-is-now",
    "title": "A year in review: vetiver",
    "section": "Where vetiver is now",
    "text": "Where vetiver is now\nVetiver also leverages the versioning and sharing capabilities of the pins package in Python and R. This package brings a straightforward way to read and write objects to different locations and allows for certain data types (namely csv and arrow files) to be passed between the Python and R language fluently.\nWithin vetiver itself, the use of VetiverModel, VetiverAPI, and monitoring helper functions gives practitioners lightweight support to bring their models to many different locations via one line deployment functions (for Posit Connect) or Dockerfile generation (for numerous on-prem or public cloud locations). These objects are able to be extended to support more advanced use cases. Vetiver is able to quickly prototype REST APIs, and then scale these prototypes safely."
  },
  {
    "objectID": "posts/state-of-vetiver/index.html#where-vetiver-is-going",
    "href": "posts/state-of-vetiver/index.html#where-vetiver-is-going",
    "title": "A year in review: vetiver",
    "section": "Where vetiver is going",
    "text": "Where vetiver is going\nFluent monitoring practices is crucial for a robust deployment. While the CI/CD in monitoring can be infrastructure dependent, it is important to close the loop between model prediction, monitoring, and retraining. Feedback loops are a place where bias in models can aggregate undetected. Any continuous monitoring support necessitates careful thought on how to uncover model (un)fairness.\nThe composability of vetiver with other projects, such as MLFlow or Metaflow, is needed to allow practitioners to build an MLOps framework that is flexible and meets the need of their team. Composability in this sense also includes platform agnosticism for public clouds such as Amazon Web Services, Azure, and Google Cloud Platform. Currently, generic Dockerfiles exist that can be hosted on these platforms, but extended documentation is needed for specific workflows."
  },
  {
    "objectID": "posts/state-of-vetiver/index.html#where-vetiver-is-not-going",
    "href": "posts/state-of-vetiver/index.html#where-vetiver-is-not-going",
    "title": "A year in review: vetiver",
    "section": "Where vetiver is not going",
    "text": "Where vetiver is not going\nDAG creation is currently out of scope of vetiver. If this is to be supported later on, it would likely end up in a new tool for maximum flexibility.\nSupport for automatic creation of model registries is not currently short-term plan. However, using pins and Quarto together can create a beautiful document to track your deployed objects. If you’re interested in this topic, this demo shows how to use pins + Quarto to track your models.\nIn all, we have learned so much from you all this year, and look forward to another year of helping data scientists bring models into production!"
  },
  {
    "objectID": "posts/gha-making-gha/gha.html",
    "href": "posts/gha-making-gha/gha.html",
    "title": "Github Actions all the way down",
    "section": "",
    "text": "The lore\nI spent a while exploring how Python packages are made. It’s the wild west out there. There’s too many setup files, testing frameworks, no agreed-upon directory structure, and generally enough information to make your head spin. Luckily, you can use tools like cookiecutter to quickly get started (once you figure out which cookiecutter you want to use). For my use case, I wanted to make a simple cookiecutter, but wanted to run some tests to make sure my configuration of files did all the things I expected it to do.\nThe workflow was:\n1. Edit my `cookiecutter`. \n2. Run my cookiecutter to make a new package. \n3. Push my new package to Github. \n4. Run Github Actions. \n5. Realize I messed up in step 1.\nBut what if, Github Actions could use the cookie cutter repo to create ANOTHER repo to automatically generate your package? And what if that freshly cookie-cut repo would run its own Github Actions to ensure everything was properly set up? Useless? Most likely. But, I was going to do it anyway.\n\n\nSee it in (Github) action\nGitHub Action that pushes to a different repo.\nGitHub Actions in receiving repo that runs once a push has been made.\n\n\nSet up\nYou will need two different repos, a sending repo and receiving repo. In my case, the sending repo generates a cookiecutter template for a Python package, and then the receiving repo is the output package. You don’t have to be familiar with cookiecutter templates to understand this gist, but they’re worth a quick click if organization brings you joy.\nBefore running your chain reaction of actions, a few pieces need to be set up. The receiving repo cannot be empty, so I just initialized it with an empty README.md. The next part is probably the most difficult if you’re unfamiliar, which is generating an SSH key, click here for instructions. Once you’ve gone into the terminal and generated the key, add the PUBLIC KEY to your SENDING repo and the PRIVATE KEY to your RECEIVING repo.\n\n\nReady, ACTION!\nThe GitHub Action that sets off this package creation is below, or you can see the action in the repo, for context. Note: there are difference between the code below and the code in the repo, these changes were made for clarity and should not impact performance.\n# Creates cookie cutter and pushes to new repo\nname: Generate Cookiecutter\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n  workflow_dispatch:\n  \njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - uses: actions/setup-python@v2\n      - name: Install requirements\n        run: pip install cookiecutter\n      - name: Make cookiecutter with default inputs\n        run: cookiecutter --no-input .\n      - name: Send to receiving repo\n        uses: peaceiris/actions-gh-pages@v3\n        with:\n          deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }}\n          external_repository: isabelizimm/making-cookies\n          publish_branch: dev-branch\n          publish_dir: ./my_new_package\n          exclude_assets: ''\nReally, the most important part here is the Send to receiving repo step. You see here that your sender deploy key is being utilized and the external repository and branch is specified. This Action will normally exclude certain files (in particular, we need .github/ from the template) on push, which is we have to manually set exclude_assets: ' ' so all files are being pushed.\nOnce this action is running successfully, you will see a new branch, dev-branch, on the receiving repo, making-cookies. Congratulations! The hard part is finished.\nThe second set of actions is less magical. This template package has built-in GitHub Actions to jump-start running tests and building docs. This second set of actions runs on push to any branch named “dev-*”, so they automatically start after the initial push to the branch from the sending repo.\nHopefully this helps (or at least gives a starting point) others who are also exploring the capabilites of CI! Feel free to contact me if you have any suggestions or struggles. :)\ncheers, iz"
  },
  {
    "objectID": "posts/were-making-a-blog/index.html",
    "href": "posts/were-making-a-blog/index.html",
    "title": "We’re making a blog (IDE agnostic edition)",
    "section": "",
    "text": "My partner, Val, has been asking to making a fancy Quarto blog. Today, we’re going through this step-by-step together. There’s a few different ways to do this, and the RStudio IDE has UI to make this a pretty slick process. However, I don’t spend much time in RStudio, so here’s my IDE agnostic workflow. You’ll need:\n\nA GitHub account\nTo install Quarto here if you haven’t yet\nAccess to the command line (eg, “Terminal” on a Mac)\n\n\nStep 1: Make the repository\nGo to github.com and click on your profile image in the top right corner. In the dropdown, click on Your repositories and then click New to create a new repository.\n\nCreate a new repository in GitHub and name it your-github-username.github.io. This is a secret repository structure in GitHub to host a blog for your account! Next, open up the terminal and use the cd command to navigate to the folder you would like to store your blog in. Use the command git clone {repository-url} to copy the repository onto your computer, and cd your-github-username.github.io into your local version.\n\n\nStep 2: Create blog\nIn the folder for your repository, type quarto create project blog . into your CLI tool and press enter. Congratulations, you have built a blog! Use quarto preview to build and view it locally.\n\n\nStep 3: Update the blog\nThere’s a few places you will want to update before you publish your blog for the world to see. First, open up the file called index.qmd and add some information about yourself. I have a few sentences of introduction, my education, and how to get in contact with me. Next, open up the folder called posts/ and update it to remove the default posts. If you’d like to write a blog post of your own, the easiest way to organize the posts is by making each one into its own folder. Then, each folder can contain a .qmd, .md, .Rmd, or .ipynb file as a post and any relevant images.\nFor simple aesthetics, you can go to the _quarto.yml file and try out different bootstrap themes, or add some color to your navbar by adding a background field to your navbar\nwebsite:\n  title: \"my blog\"\n  description: \"my personal blog\"\n  navbar:\n    background: \"#ffccff\" # accepts color names like red, blue, or hex codes\n\n\nStep 4: Push to Github Pages\nWhile still in the folder of your blog, run quarto publish gh-pages. You will be prompted with some text like: Publish site to your-github-username.github.io? Accept this, and let it run.\n\n\nStep 5: Set the deployment branch to gh-pages\nBack to GitHub, view your repository. Go to the settings for the repo, and click on Pages. Then, change the Build and deploy from source to the gh-pages branch.\n\nIf you go back to the main page of your repository, you will see a tab Deployments underneath the repository description. The green checkmark means your blog has been successfully deployed! The url will be something like your-github-username.github.io\nGoing forward, each time you want to update your website, you can reuse the quarto publish gh-pages command."
  },
  {
    "objectID": "talks/rstudioconf2022/index.html",
    "href": "talks/rstudioconf2022/index.html",
    "title": "Demystifying MLOps",
    "section": "",
    "text": "Data scientists have an intuition of what goes into training a machine learning model, but building an MLOps strategy to deploy that model can sound daunting for data science teams. Model services are not one-size-fits-all, so it is imperative to know a range of tools available. One option, Vetiver, is a framework for R and Python created to make model deployment feel like a natural extension of a data scientist’s skill set.\nThis talk offers a high-level overview of what MLOps options are available for model operationalization, but also shows a practical example of an end-to-end MLOps deployment of a model-aware REST API using Vetiver.\nView the repository || Watch the recording\n\n\n\nThis was SUCH a fun conference to go to! The warmth and sense of community in the R world is so apparent; they were pretty accepting of me, a Pythonista 😉. This talk brings the ways you can deploy models into the mechanics of baking cookies.\nP.S. Here is the shirt I wear in this talk, which reads, “Rage against the Machine Learning”. This is not an affiliate link (but it probably should be after this shirt got so much love at the conference)."
  },
  {
    "objectID": "talks/rstudioconf2022/index.html#abstract",
    "href": "talks/rstudioconf2022/index.html#abstract",
    "title": "Demystifying MLOps",
    "section": "",
    "text": "Data scientists have an intuition of what goes into training a machine learning model, but building an MLOps strategy to deploy that model can sound daunting for data science teams. Model services are not one-size-fits-all, so it is imperative to know a range of tools available. One option, Vetiver, is a framework for R and Python created to make model deployment feel like a natural extension of a data scientist’s skill set.\nThis talk offers a high-level overview of what MLOps options are available for model operationalization, but also shows a practical example of an end-to-end MLOps deployment of a model-aware REST API using Vetiver.\nView the repository || Watch the recording\n\n\n\nThis was SUCH a fun conference to go to! The warmth and sense of community in the R world is so apparent; they were pretty accepting of me, a Pythonista 😉. This talk brings the ways you can deploy models into the mechanics of baking cookies.\nP.S. Here is the shirt I wear in this talk, which reads, “Rage against the Machine Learning”. This is not an affiliate link (but it probably should be after this shirt got so much love at the conference)."
  },
  {
    "objectID": "talks/pydataglobal2022/index.html",
    "href": "talks/pydataglobal2022/index.html",
    "title": "Practical MLOps for better models",
    "section": "",
    "text": "Machine learning operations (MLOps) are often synonymous with large and complex applications, but many MLOps practices help practitioners build better models, regardless of the size. This talk shares best practices for operationalizing a model and practical examples using the open-source MLOps framework vetiver to version, share, deploy, and monitor models.\nView the repository || Watch the recording\n\n\n\nI will always remember PyData Global 2022 as the conference that Hadley Wickham, who is essentially the face of the R language, talked about Python. It was a delightful crossover episode in my world. My talk really was an ode to how I learned about MLOps. It goes through what I thought data science looked like when I learned about it in school, only to be SHOCKED that I needed a little bit of practical MLOps knowledge to collaborate with teammates, not lose models into the abyss of version1, version2, version2_final, version_final_forreal, and overall be able to do my job effectively."
  },
  {
    "objectID": "talks/pydataglobal2022/index.html#abstract",
    "href": "talks/pydataglobal2022/index.html#abstract",
    "title": "Practical MLOps for better models",
    "section": "",
    "text": "Machine learning operations (MLOps) are often synonymous with large and complex applications, but many MLOps practices help practitioners build better models, regardless of the size. This talk shares best practices for operationalizing a model and practical examples using the open-source MLOps framework vetiver to version, share, deploy, and monitor models.\nView the repository || Watch the recording\n\n\n\nI will always remember PyData Global 2022 as the conference that Hadley Wickham, who is essentially the face of the R language, talked about Python. It was a delightful crossover episode in my world. My talk really was an ode to how I learned about MLOps. It goes through what I thought data science looked like when I learned about it in school, only to be SHOCKED that I needed a little bit of practical MLOps knowledge to collaborate with teammates, not lose models into the abyss of version1, version2, version2_final, version_final_forreal, and overall be able to do my job effectively."
  },
  {
    "objectID": "talks/crunch2022/index.html",
    "href": "talks/crunch2022/index.html",
    "title": "Building an MLOps strategy from the ground up",
    "section": "",
    "text": "Watch the recording\n\n\nPost talk notes\nThis was the longest talk I had ever given at 60 minutes, and, on a really personal note, my first keynote presentation. I was so scared, but I came ready with lots of content that I was really excited to share and was welcomed warmly by the whole Crunch crew. The audience was great, and there were lots of questions about the tricks I taught my dog (which, much like my models, performed well in my living room but not so well in the real world), but also about MLOps and vetiver. This talk is good to listen to if you’d like to learn more about:\n\nthe tension MLOps creates between software engineering and data science workflows\nwhen and where you can version models with pins\nwhy your DevOps friends might be confused when you say you’re monitoring a model\nhow to write good models (from a fairness perspective)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "isabel zimmerman",
    "section": "",
    "text": "Isabel Zimmerman is a software engineer at Posit, PBC, where she builds Python-based MLOps frameworks. When not thinking about machine learning systems, she enjoys gardening and competing in agility competitions with her dog, Toast."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "isabel zimmerman",
    "section": "Education",
    "text": "Education\nFlorida Polytechnic University | Lakeland, FL\nMS in Computer Science | Aug 2021 - May 2023\nFlorida Polytechnic University | Lakeland, FL\nBS in Data Science | Aug 2017 - May 2021"
  }
]
[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "isabel zimmerman",
    "section": "",
    "text": "A gentle introduction to f-strings\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a portfolio website in Positron\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLet AI do your coding laundry\n\n\nHow I use AI to minimize Levels of Possible Harm.\n\n\n\nai\n\n\n\n\n\n\n\n\n\nSep 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Started with Cline in Positron\n\n\n\n\n\n\nai\n\n\n\nA quick guide to using the Cline AI assistant in Positron.\n\n\n\n\n\nMar 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nReading Time Calculator\n\n\n\n\n\n\nfun\n\n\n\nCalculate the time remaining in an audiobook.\n\n\n\n\n\nJan 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCookie reviews: chocolate chocolate chip\n\n\n\n\n\n\ncookies\n\n\n\n4.5/5 ‚≠ê double chocolate and YUM!\n\n\n\n\n\nJan 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPlotnine plot contest\n\n\n\n\n\n\npython\n\n\n\nA very cheesy map made with plotnine\n\n\n\n\n\nJun 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCookie reviews: chocolate chip with cornstarch and heavy cream\n\n\n\n\n\n\ncookies\n\n\n\n3.75/5 ‚≠ê very sweet, very chewy\n\n\n\n\n\nJun 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAbstract art\n\n\n\n\n\n\nconferences\n\n\n\nHow to create an abstract to inform and delight.\n\n\n\n\n\nMay 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThe right tools to learn (my first experience using Spline)\n\n\n\n\n\n\nfun\n\n\n\nSometimes, you don‚Äôt need the best tool for the job, you need the best tool for you.\n\n\n\n\n\nApr 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSo, you want to learn about MLOps\n\n\n\n\n\n\nmlops\n\n\n\nMy recommendations to learning about MLOps\n\n\n\n\n\nApr 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBlind date with a data talk\n\n\n\n\n\n\nconferences\n\nfun\n\n\n\nPick a mood, watch a talk!\n\n\n\n\n\nMar 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWe‚Äôre making a blog (IDE agnostic edition)\n\n\n\n\n\n\nquarto\n\n\n\nMaking a Quarto blog and deploying it on GitHub Pages, step by step.\n\n\n\n\n\nSep 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nA year in review: vetiver\n\n\n\n\n\n\nmlops\n\nr\n\npython\n\n\n\n\n\n\n\n\n\nOct 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPyenv in RStudio\n\n\n\n\n\n\npython\n\n\n\nUsing pyenv in the RStudio IDE.\n\n\n\n\n\nSep 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple models on one API\n\n\n\n\n\n\nmlops\n\n\n\nDeploying multiple models to one VetiverAPI.\n\n\n\n\n\nSep 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nGithub Actions all the way down\n\n\n\n\n\n\ngit\n\nfun\n\n\n\nUsing Github Actions to create new repos that also run their own Github Actions.\n\n\n\n\n\nMar 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nCat detection with explainers\n\n\n\n\n\n\npython\n\n\n\nAnchor explainers on images to better understand model decision-making.\n\n\n\n\n\nMar 20, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Isabel Zimmerman",
    "section": "",
    "text": "Hi there! I‚Äôm Isabel. I love creating things, whether that be making DIY crafts or open source tools. I‚Äôm keenly interested in making Python an easier and more fun place to do data science work.\nI‚Äôve spent time building Python packages like vetiver and pins as well as larger tools like the Positron IDE. My favorite type of work tends to be at the intersection of practical, user-facing tools and data science. I‚Äôm part of the pyOpenSci community, serving as an editor, reviewer, and Editor-in-Chief emeritus (Winter 2024 rotation). I enjoy sharing my work through blog posts and conference talks. I‚Äôve also compiled a more in-depth list of my work.\nCoding is a place for me to be whimsical and creative, but I tend to also do a lot of non-computer hobbies! I‚Äôm an avid reader, amateur book-binder, constant doodler (I drew my entire landing page!), and am always teaching my old dog new tricks."
  },
  {
    "objectID": "talks/2025-flpoly-quarto/index.html#end-goal",
    "href": "talks/2025-flpoly-quarto/index.html#end-goal",
    "title": "0 to portfolio website",
    "section": "End goal",
    "text": "End goal"
  },
  {
    "objectID": "talks/2025-flpoly-quarto/index.html#what-is-github",
    "href": "talks/2025-flpoly-quarto/index.html#what-is-github",
    "title": "0 to portfolio website",
    "section": "What is GitHub?",
    "text": "What is GitHub?\n\nversion control: track changes and revert to previous versions easily.\ncollaboration: work with others without overwriting code.\nportfolio building: show coding skills to employers.\nopen-source world: contribute to real-world projects and communities.\nproject management: organize tasks and track progress."
  },
  {
    "objectID": "talks/2025-flpoly-quarto/index.html#what-is-quarto",
    "href": "talks/2025-flpoly-quarto/index.html#what-is-quarto",
    "title": "0 to portfolio website",
    "section": "What is Quarto?",
    "text": "What is Quarto?\n\nliterate programming: Create reports, blogs, and presentations with code and text.\nmany formats: Export to PDF, HTML, Word, Powerpoint, and more."
  },
  {
    "objectID": "talks/2025-flpoly-quarto/index.html#what-youll-need",
    "href": "talks/2025-flpoly-quarto/index.html#what-youll-need",
    "title": "0 to portfolio website",
    "section": "What you‚Äôll need",
    "text": "What you‚Äôll need\n\nA GitHub profile\nQuarto\nEither Positron or VSCode + Quarto extension\n\n\n\n\n‚àí+\n05:00"
  },
  {
    "objectID": "talks/2025-flpoly-quarto/index.html#get-git-on-your-computer",
    "href": "talks/2025-flpoly-quarto/index.html#get-git-on-your-computer",
    "title": "0 to portfolio website",
    "section": "Get git on your computer",
    "text": "Get git on your computer\n\nInstall git on your computer\n\nOnce installed, run:\ngit config --global user.name \"github_username\"\ngit config --global user.email \"your_email@example.com\"\n\n\n\n\n‚àí+\n05:00"
  },
  {
    "objectID": "talks/2025-flpoly-quarto/index.html#create-repository-for-your-blog",
    "href": "talks/2025-flpoly-quarto/index.html#create-repository-for-your-blog",
    "title": "0 to portfolio website",
    "section": "Create repository for your blog",
    "text": "Create repository for your blog\nCreate a new repository with the name: your-username.github.io\n\n\n\n\n‚àí+\n02:00"
  },
  {
    "objectID": "talks/2025-flpoly-quarto/index.html#clone-repository",
    "href": "talks/2025-flpoly-quarto/index.html#clone-repository",
    "title": "0 to portfolio website",
    "section": "Clone repository",
    "text": "Clone repository"
  },
  {
    "objectID": "talks/2025-flpoly-quarto/index.html#clone-repository-1",
    "href": "talks/2025-flpoly-quarto/index.html#clone-repository-1",
    "title": "0 to portfolio website",
    "section": "Clone repository",
    "text": "Clone repository\n# use your copied repo name!\ngit clone https://github.com/isabelizimm/isabelizimm.github.io.git\n\nRun this command in the terminal in the directory you would like your website.\nFor me, I use something like ~/code/isabelizimm.github.io\n\n\n\n\n‚àí+\n02:00"
  },
  {
    "objectID": "talks/2025-flpoly-quarto/index.html#create-blog",
    "href": "talks/2025-flpoly-quarto/index.html#create-blog",
    "title": "0 to portfolio website",
    "section": "Create blog!",
    "text": "Create blog!\n\nUse command+shift+p to open the Command Palette\nIf you don‚Äôt have a command key, use ctrl instead\nQuarto: Create Project -&gt; Blog\n\n\n\n\n\n‚àí+\n02:00"
  },
  {
    "objectID": "talks/2025-flpoly-quarto/index.html#change-_quarto.yml-file",
    "href": "talks/2025-flpoly-quarto/index.html#change-_quarto.yml-file",
    "title": "0 to portfolio website",
    "section": "Change _quarto.yml file",
    "text": "Change _quarto.yml file\n\n\n\n\n‚àí+\n01:00"
  },
  {
    "objectID": "talks/2025-flpoly-quarto/index.html#update-about.qmd",
    "href": "talks/2025-flpoly-quarto/index.html#update-about.qmd",
    "title": "0 to portfolio website",
    "section": "Update about.qmd",
    "text": "Update about.qmd\n\n\n\n\n‚àí+\n02:00"
  },
  {
    "objectID": "talks/2025-flpoly-quarto/index.html#check-out-your-beautiful-blog",
    "href": "talks/2025-flpoly-quarto/index.html#check-out-your-beautiful-blog",
    "title": "0 to portfolio website",
    "section": "Check out your beautiful blog!",
    "text": "Check out your beautiful blog!\n\nClick Preview button in top right corner OR run quarto preview in terminal"
  },
  {
    "objectID": "talks/2025-flpoly-quarto/index.html#send-your-source-code-to-github",
    "href": "talks/2025-flpoly-quarto/index.html#send-your-source-code-to-github",
    "title": "0 to portfolio website",
    "section": "Send your source code to GitHub",
    "text": "Send your source code to GitHub\ngit add .\ngit commit -m \"explain what you did\"\ngit push"
  },
  {
    "objectID": "talks/2025-flpoly-quarto/index.html#publish-blog-part-1",
    "href": "talks/2025-flpoly-quarto/index.html#publish-blog-part-1",
    "title": "0 to portfolio website",
    "section": "Publish blog, part 1",
    "text": "Publish blog, part 1\n\nIn the terminal, run quarto publish gh-pages\n\n\n\n\n‚àí+\n01:00"
  },
  {
    "objectID": "talks/2025-flpoly-quarto/index.html#publish-blog-part-2",
    "href": "talks/2025-flpoly-quarto/index.html#publish-blog-part-2",
    "title": "0 to portfolio website",
    "section": "Publish blog, part 2",
    "text": "Publish blog, part 2\n\nGo to your GitHub repo -&gt; Settings -&gt; Pages\nChoose Deploy from branch and set the branch to gh-pages\n\n\n\n\n\n‚àí+\n02:00"
  },
  {
    "objectID": "talks/2025-flpoly-quarto/index.html#what-is-a-quarto-markdown-file",
    "href": "talks/2025-flpoly-quarto/index.html#what-is-a-quarto-markdown-file",
    "title": "0 to portfolio website",
    "section": "What is a Quarto markdown file?",
    "text": "What is a Quarto markdown file?\n\nText\nCode (either static or run on render)\nShow examples\n\n```python\n1+1\n```"
  },
  {
    "objectID": "talks/2025-flpoly-quarto/index.html#where-can-i-edit-the-look-of-a-website",
    "href": "talks/2025-flpoly-quarto/index.html#where-can-i-edit-the-look-of-a-website",
    "title": "0 to portfolio website",
    "section": "Where can I edit the look of a website?",
    "text": "Where can I edit the look of a website?\n\nPrimarily in the _quarto.yml file\nCheck out docs to see themes or other customization\nLet‚Äôs do a mini walkthrough of the site"
  },
  {
    "objectID": "talks/2025-flpoly-quarto/index.html#feel-free-to-edit-away",
    "href": "talks/2025-flpoly-quarto/index.html#feel-free-to-edit-away",
    "title": "0 to portfolio website",
    "section": "Feel free to edit away!",
    "text": "Feel free to edit away!"
  },
  {
    "objectID": "talks/crunch2022/index.html",
    "href": "talks/crunch2022/index.html",
    "title": "Building an MLOps strategy from the ground up",
    "section": "",
    "text": "Watch the recording\n\n\nPost talk notes\nThis was the longest talk I had ever given at 60 minutes, and, on a really personal note, my first keynote presentation. I was so scared, but I came ready with lots of content that I was really excited to share and was welcomed warmly by the whole Crunch crew. The audience was great, and there were lots of questions about the tricks I taught my dog (which, much like my models, performed well in my living room but not so well in the real world), but also about MLOps and vetiver. This talk is good to listen to if you‚Äôd like to learn more about:\n\nthe tension MLOps creates between software engineering and data science workflows\nwhen and where you can version models with pins\nwhy your DevOps friends might be confused when you say you‚Äôre monitoring a model\nhow to write good models (from a fairness perspective)"
  },
  {
    "objectID": "talks/pydataglobal2022/index.html",
    "href": "talks/pydataglobal2022/index.html",
    "title": "Practical MLOps for better models",
    "section": "",
    "text": "Machine learning operations (MLOps) are often synonymous with large and complex applications, but many MLOps practices help practitioners build better models, regardless of the size. This talk shares best practices for operationalizing a model and practical examples using the open-source MLOps framework vetiver to version, share, deploy, and monitor models.\nView the repository || Watch the recording\n\n\n\nI will always remember PyData Global 2022 as the conference that Hadley Wickham, who is essentially the face of the R language, talked about Python. It was a delightful crossover episode in my world. My talk really was an ode to how I learned about MLOps. It goes through what I thought data science looked like when I learned about it in school, only to be SHOCKED that I needed a little bit of practical MLOps knowledge to collaborate with teammates, not lose models into the abyss of version1, version2, version2_final, version_final_forreal, and overall be able to do my job effectively."
  },
  {
    "objectID": "talks/pydataglobal2022/index.html#abstract",
    "href": "talks/pydataglobal2022/index.html#abstract",
    "title": "Practical MLOps for better models",
    "section": "",
    "text": "Machine learning operations (MLOps) are often synonymous with large and complex applications, but many MLOps practices help practitioners build better models, regardless of the size. This talk shares best practices for operationalizing a model and practical examples using the open-source MLOps framework vetiver to version, share, deploy, and monitor models.\nView the repository || Watch the recording\n\n\n\nI will always remember PyData Global 2022 as the conference that Hadley Wickham, who is essentially the face of the R language, talked about Python. It was a delightful crossover episode in my world. My talk really was an ode to how I learned about MLOps. It goes through what I thought data science looked like when I learned about it in school, only to be SHOCKED that I needed a little bit of practical MLOps knowledge to collaborate with teammates, not lose models into the abyss of version1, version2, version2_final, version_final_forreal, and overall be able to do my job effectively."
  },
  {
    "objectID": "talks/rstudioconf2022/index.html",
    "href": "talks/rstudioconf2022/index.html",
    "title": "Demystifying MLOps",
    "section": "",
    "text": "Data scientists have an intuition of what goes into training a machine learning model, but building an MLOps strategy to deploy that model can sound daunting for data science teams. Model services are not one-size-fits-all, so it is imperative to know a range of tools available. One option, Vetiver, is a framework for R and Python created to make model deployment feel like a natural extension of a data scientist‚Äôs skill set.\nThis talk offers a high-level overview of what MLOps options are available for model operationalization, but also shows a practical example of an end-to-end MLOps deployment of a model-aware REST API using Vetiver.\nView the repository || Watch the recording\n\n\n\nThis was SUCH a fun conference to go to! The warmth and sense of community in the R world is so apparent; they were pretty accepting of me, a Pythonista üòâ. This talk brings the ways you can deploy models into the mechanics of baking cookies.\nP.S. Here is the shirt I wear in this talk, which reads, ‚ÄúRage against the Machine Learning‚Äù. This is not an affiliate link (but it probably should be after this shirt got so much love at the conference)."
  },
  {
    "objectID": "talks/rstudioconf2022/index.html#abstract",
    "href": "talks/rstudioconf2022/index.html#abstract",
    "title": "Demystifying MLOps",
    "section": "",
    "text": "Data scientists have an intuition of what goes into training a machine learning model, but building an MLOps strategy to deploy that model can sound daunting for data science teams. Model services are not one-size-fits-all, so it is imperative to know a range of tools available. One option, Vetiver, is a framework for R and Python created to make model deployment feel like a natural extension of a data scientist‚Äôs skill set.\nThis talk offers a high-level overview of what MLOps options are available for model operationalization, but also shows a practical example of an end-to-end MLOps deployment of a model-aware REST API using Vetiver.\nView the repository || Watch the recording\n\n\n\nThis was SUCH a fun conference to go to! The warmth and sense of community in the R world is so apparent; they were pretty accepting of me, a Pythonista üòâ. This talk brings the ways you can deploy models into the mechanics of baking cookies.\nP.S. Here is the shirt I wear in this talk, which reads, ‚ÄúRage against the Machine Learning‚Äù. This is not an affiliate link (but it probably should be after this shirt got so much love at the conference)."
  },
  {
    "objectID": "work.html",
    "href": "work.html",
    "title": "isabel zimmerman",
    "section": "",
    "text": "Open source work\n\nCurrently working on the Positron IDE.\nvetiver, Python package. MLOps framework for versioning, deploying, and monitoring models. Creator, maintainer. 2021-present.\npins, Python package. Publish data, models, and other Python objects to different backends. Maintainer. 2023-present.\npaintnine, Python package. Make abstract art based off of plotnine. Creator. 2022.\npyOpenSci, Open source organization. Review and guide others through peer-reviewing Python packages. Editor emeritus and reviewer, 2023-2025. Editor-in-Chief Emeritus, Winter 2024.\nOther contributions to projects such as Cline, plotnine, great-tables, and more. See my GitHub.\n\n\n\nConference talks\n\nPyData Boston 2025 keynote: A toolkit for building and using tools\nposit::conf() 2025: IDE-ntity crisis: Choosing the right tool for you\nPyData NYC 2024: End-to-end data science with the Positron IDE\nSciPy 2024: From code to clarity, using Quarto for Python Documentation\nposit::conf() 2024: Exploratory Data Analysis with Python in Positron\nposit::conf() 2023: Thanks, I made it in quartodoc\nPyData NYC 2022: Holistic MLOps for better science\nCrunch 2022: Building an MLOps strategy from the ground up\nPyData Global 2022: Practical MLOps for better models\nBerlin Buzzwords 2021: Explaining model explainability\n\n\n\nOther speaking engagements\n\nThe Legacy Podcast by Florida Poly: podcast guest, 2026. Recording forthcoming.\nData Science Hangout 2025: Open source development practices, co-presented with Davis Vaughan\nData Science Lab by Posit 2025: Exploring Positron settings, co-presented with Davis Vaughan\nUnlocking My Potential: podcast guest, 2025.\nFlorida Poly DSBAA meetup 2025 Workshop: 0 to portfolio website, co-taught with Valentina Colorado\nposit::conf() 2024 Workshop: Intro to MLOps with vetiver, co-taught with Julia Silge\nR-Ladies Rome Meetup 2024: Extending the data science workflow: {vetiver} and {pins}\nRStudio Meetup 2022: MLOps with vetiver in Python and R, co-presented with Julia Silge\nO‚ÄôReilly AI Superstream: MLOps 2022: Composable Tools for Robust MLOps Deployment\n\n\n\nOutreach and other\n\nFlorida Poly Women+ Venture committee member, 2025-present.\nBerlin Buzzwords program committee, 2026.\n\n\n\nEducation\n\nBachelors of Science, Data Science from Florida Polytechnic University. Concentrated in quantitative economics and econometrics. Completed May 2021.\nMasters of Science, Computer Science from Florida Polytechnic University. Concentrated in data science. Completed May 2023.\n\n\n\nWritten work\n\nReal Python tutorial writer. See my portfolio.\nEvaluating the MLOps readiness of your team. Published on the Posit blog.\nMeta-Analysis of the Machine Learning Operations Open Source Ecosystem. 2023 International Conference on Machine Learning and Applications (ICMLA)."
  },
  {
    "objectID": "posts/chocolate-cookie/index.html",
    "href": "posts/chocolate-cookie/index.html",
    "title": "Cookie reviews: chocolate chocolate chip",
    "section": "",
    "text": "4.5/5 ‚≠ê. This is my own recipe, so I am unashamedly bias. I love anything chocolatey. This lovely because 1) I almost always have all the ingredients on hand and 2) it gives you all the gooey goodness of a brownie in about 15 minutes flat. The measurements are a bit odd (and I have mismeasured what 2/3c of butter is more times than I‚Äôd like to admit). Despite its funky ratios, it is my go-to.\nMakes about 10 cookies. \n\n2/3c butter, softened\n1/2 + 1/3c white sugar\n1 teaspoon vanilla\n1 egg\n1/2 teaspoon salt\n1/2 cup cocoa powder\n1/2 teaspoon baking soda\n1 cup flour\n3/4 cup chocolate chips\n\n0. Preheat oven to 350F.\n1. Mix butter and sugar until fluffy.\n2. Stir in vanilla and egg until combined.\n3. Add salt, cocoa powder, baking soda, flour. Stir until combined.\n4. Fold in chocolate chips.\n5. Bake in oven for 8-ish mins. They will crack on top, but should still be soft."
  },
  {
    "objectID": "posts/cline_positron/index.html",
    "href": "posts/cline_positron/index.html",
    "title": "Getting Started with Cline in Positron",
    "section": "",
    "text": "If you‚Äôve been looking for a way to integrate AI assistance directly into your coding workflow in Positron, Cline might be just what you need."
  },
  {
    "objectID": "posts/cline_positron/index.html#setting-up-cline",
    "href": "posts/cline_positron/index.html#setting-up-cline",
    "title": "Getting Started with Cline in Positron",
    "section": "Setting Up Cline",
    "text": "Setting Up Cline\n\nOpen Positron and navigate to the Extensions view (Ctrl+Shift+X or Cmd+Shift+X on Mac)\nSearch for Cline by its extension id: ‚Äúsaoudrizwan.claude-dev‚Äù\nYou‚Äôll need to authenticate with your Claude API key (or sign up if you don‚Äôt have one)\nOnce authenticated, you‚Äôre ready to go!"
  },
  {
    "objectID": "posts/cline_positron/index.html#features-worth-knowing",
    "href": "posts/cline_positron/index.html#features-worth-knowing",
    "title": "Getting Started with Cline in Positron",
    "section": "Features Worth Knowing",
    "text": "Features Worth Knowing\n\nContext-Aware Assistance\nUnlike using Claude in a browser, Cline in Positron understands your code context. It can see your open files, project structure, and even terminal output. This means you get more relevant suggestions and help.\n\n\nDirect File Operations\nCline can create, read, and modify files directly in your project.\n\n\nTerminal Integration\nYou can have Cline run commands in your terminal, check the results, and make adjustments based on what happens. This is incredibly useful for debugging, running tests, or setting up project dependencies.\n\n\nCode Search\nInstead of trying to remember where that specific function was defined or using regex searches, ask in plain English."
  },
  {
    "objectID": "posts/cline_positron/index.html#tips-for-better-results",
    "href": "posts/cline_positron/index.html#tips-for-better-results",
    "title": "Getting Started with Cline in Positron",
    "section": "Tips for Better Results",
    "text": "Tips for Better Results\n\nBe specific in your requests\nOpen editors that might be relevant for your task\nUse follow-up questions to refine responses\nRemember that Cline can see your project structure, so you can reference files by name\n\n\nAdd custom instructions\n\nCustom instructions allow you to personalize how Cline interacts with your code and responds to your queries. By setting up custom instructions, you can:\n\nDefine your preferred coding style and conventions\nSpecify the programming languages and frameworks you commonly use\nSet the tone and verbosity level of responses\nInclude project-specific context that Cline should be aware of\n\n\n\nUse a Memory Bank\nMemory banks in Cline allow you to store important context that persists across sessions. This feature is useful for:\n\nSaving project-specific knowledge that you don‚Äôt want to repeat in every prompt\nMaintaining context about your codebase architecture and design decisions\nStoring frequently used code snippets or patterns specific to your project\nPreserving the history of complex problem-solving discussions\nCreating a knowledge base of project-specific terminology and conventions\n\nUnlike regular conversations that have context limitations, memory banks ensure that critical information remains available to Cline whenever you need it, improving consistency and reducing repetition."
  },
  {
    "objectID": "posts/cline_positron/index.html#model-comparison",
    "href": "posts/cline_positron/index.html#model-comparison",
    "title": "Getting Started with Cline in Positron",
    "section": "Model comparison",
    "text": "Model comparison\nCline has put together a model comparison for what they consider the best models to use, these are the results as of February 2025."
  },
  {
    "objectID": "posts/cline_positron/index.html#jupyter-notebook-limitations",
    "href": "posts/cline_positron/index.html#jupyter-notebook-limitations",
    "title": "Getting Started with Cline in Positron",
    "section": "Jupyter Notebook Limitations",
    "text": "Jupyter Notebook Limitations\nCline has some limitations when working with Jupyter notebooks. In particular, Cline cannot directly analyze outputs in notebook cells, including visualizations, tables, or interactive widgets. Additionally, unlike native notebook tools, Cline cannot inspect variable values or data structures at runtime. These limitations are important to consider if your workflow heavily involves data analysis in notebooks with frequent output examination."
  },
  {
    "objectID": "posts/cline_positron/index.html#final-thoughts",
    "href": "posts/cline_positron/index.html#final-thoughts",
    "title": "Getting Started with Cline in Positron",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nOne of the downfalls using this extension is that Cline cannot reach into the Console, Variables, Plots, Connections, Viewer, or Help panes in Positron. If you‚Äôre someone who wants to analyze output in these locations, this may not be the right fit for you. Despite these limitations, Cline excels at code generation, refactoring, and understanding project structure. That being said, having an AI assistant directly in your editor can be useful. It‚Äôs easy to set up, so I think it‚Äôs worth giving it a try!\nDisclosure: the article was written by me and edited with AI using the Cline extension."
  },
  {
    "objectID": "posts/f-strings/index.html",
    "href": "posts/f-strings/index.html",
    "title": "A gentle introduction to f-strings",
    "section": "",
    "text": "Formatted String Literals, called f-strings for short, are a way for you to include expressions and format values inside of a Python string. F-strings are available in Python 3.6+ (earlier versions use format()).\nTo create an f-string, put an f in front of your string literal, then use {} curly braces to add variables or expressions. Here‚Äôs how you create a basic f-string:\nYou can see the variable x is embedded directly into the string, producing 'A century is 100 years'."
  },
  {
    "objectID": "posts/f-strings/index.html#using-expressions-inside-f-strings",
    "href": "posts/f-strings/index.html#using-expressions-inside-f-strings",
    "title": "A gentle introduction to f-strings",
    "section": "Using Expressions Inside F-Strings",
    "text": "Using Expressions Inside F-Strings\nHere‚Äôs where f-strings get really powerful: you can put any Python expression inside the curly braces, not just variables. This means you can do calculations, call functions, or use methods directly inside your string.\nHere‚Äôs an example with arithmetic:\n&gt;&gt;&gt; slices_per_pizza = 8\n&gt;&gt;&gt; pizzas = 3\n&gt;&gt;&gt; total_pizza = f\"Total pizza slices: {slices_per_pizza * pizzas}\"\n&gt;&gt;&gt; print(total_pizza)\n'Total pizza slices: 24'\nThe expression slices_per_pizza * pizzas is evaluated when the f-string is created, giving you 'Total pizza slices: 24'. You can also call string methods to transform text on the fly:\n&gt;&gt;&gt; magic_spell = \"abracadabra\"\n&gt;&gt;&gt; wizard = f\"The wizard cast {magic_spell.upper()}!\"\n&gt;&gt;&gt; print(wizard)\n'The wizard cast ABRACADABRA!'\nThis converts the spell‚Äôs name to uppercase, producing 'The wizard cast ABRACADABRA!'. ## Formatting Values\nOne of f-strings‚Äô most useful features is the ability to control how values are displayed. You add formatting instructions by adding a colon : after the variable name inside the curly braces, giving you precise control over your output.\nYou control decimal places with .Nf where N is the number of digits. This is particularly important when working with financial data or scientific calculations:\n&gt;&gt;&gt; import math\n&gt;&gt;&gt; pi_digits = f'The first 10 digits of pi are {math.pi:.10f}'\n&gt;&gt;&gt; print(pi_digits)\n'The first 10 digits of pi are 3.1415926536'\nThis limits pi to 10 decimal places: 'The first 10 digits of pi are 3.1415926536'.\n\nDate Formatting\nWhen working with datetime objects, you can use format codes after the colon. Common codes include %B for full month name, %d for day, and %Y for full year:\n&gt;&gt;&gt; from datetime import datetime\n&gt;&gt;&gt; square_date = datetime(2025, 9, 16)\n&gt;&gt;&gt; formatted_date = f\"Date: {square_date:%B %d, %Y}\"\n&gt;&gt;&gt; print(formatted_date)\n'Date: September 16, 2025'\nThis produces a nicely formatted date: 'Date: September 16, 2025'."
  },
  {
    "objectID": "posts/f-strings/index.html#special-conversions",
    "href": "posts/f-strings/index.html#special-conversions",
    "title": "A gentle introduction to f-strings",
    "section": "Special Conversions",
    "text": "Special Conversions\nUse !r to see the string representation of a value. This shows quotes around strings and escape characters, which is helpful for debugging:\n&gt;&gt;&gt; name = \"Marco\\nPolo\"\n&gt;&gt;&gt; f\"Repr: {name!r}\"\n\"Repr: 'Marco\\\\nPolo'\"\nHere‚Äôs what you get: \"Repr: 'Marco\\\\nPolo'\". The newline character is displayed as \\\\n instead of being interpreted. The !a conversion works like !r but uses ascii() to escape non-ASCII characters.\nYou can also use = after your expression to show both the expression and its value:\n&gt;&gt;&gt; movie_rating = 8.7\n&gt;&gt;&gt; f\"The value of {movie_rating=}\"\n'The value of movie_rating=8.7'\nWhere the output This saves you from typing the variable name twice.\nF-strings are the modern, preferred way to format strings in Python. They make your code more readable and string output more polished."
  },
  {
    "objectID": "posts/spline/index.html",
    "href": "posts/spline/index.html",
    "title": "The right tools to learn (my first experience using Spline)",
    "section": "",
    "text": "I found out about Spline from a recommended video on Instagram. It was marketed as Blender for dummies. I don‚Äôt know anything about game design. My complete knowledge of Blender was that it was good for smoothies and also animation(?). But, I like to think I‚Äôm a sort of creative person. I like to learn and make new things. I was certainly a dummy in this context. Why not try it out?\nI logged in (it‚Äôs freemium software), and opened up one of the templates provided. There were many instructional videos and tutorials to teach you how to use the software, but I went in headfirst and hoped for the best. If you have used Photoshop-y applications before, the layout might look vaguely reminiscient. There‚Äôs a pane on the left showing all the objects in a scene, and a pane on the right to edit a selected object. There‚Äôs buttons on top to drop you into the world, add new shapes, and easily export your scene.\nHere‚Äôs my ‚ÄúThe Intersection‚Äù that shows a split between two ecosystems. Walk around, flip over the bench, stand on some dinosaur bones, fall off the edge into eternal darkness, there is no plot! Click on the graphic, then you can use the arrow keys to change the camera and WASD keys to move the bunny plus space bar to jump.\n\nSpline is not the software where Fortnite is developed. But, it doesn‚Äôt need to be. It‚Äôs a delightful tool that is easily understood, has a simple UI that doesn‚Äôt require you to know everything to be productive, and offers great templates to build from. My goal was never to change career paths into game design; my goal was to make something that isn‚Äôt too embarrassingly bad to put in a blog post and have some fun along the way.\nAnyway, like most software, Spline is a good example of the fact that sometimes you don‚Äôt need the best tool for the job, you need the best tool for you."
  },
  {
    "objectID": "posts/cornstach-cream-cookie/index.html",
    "href": "posts/cornstach-cream-cookie/index.html",
    "title": "Cookie reviews: chocolate chip with cornstarch and heavy cream",
    "section": "",
    "text": "3.75/5 ‚≠ê. Besides my general annoyance with any recipe that involves a mixer1, this is a decent cookie. They‚Äôre quite sweet and very chewy. The amount of dairy in this cookie is enough that lactose-intolerant people need a warning. I don‚Äôt think this recipe is worth buying heavy cream and cornstarch, but if you already own these ingredients, no one is going to complain if you whip up a batch of these bad boys.\nThis recipe was found on Instagram, credits to @samanttha_chen."
  },
  {
    "objectID": "posts/cornstach-cream-cookie/index.html#footnotes",
    "href": "posts/cornstach-cream-cookie/index.html#footnotes",
    "title": "Cookie reviews: chocolate chip with cornstarch and heavy cream",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI don‚Äôt own a stand up mixer. I use a hand mixer that resides in the back corner of a dark cabinet in it‚Äôs massive original case for ‚Äúeasier storage‚Äù even though I can‚Äôt fit the attachments in the case, so they live in a few different locations, depending on who emptied out the dishwasher. Anyway, the hand mixer is mostly a pain to get out. There‚Äôs a better organizational system there, but I am so used to my broken workflow that it‚Äôs almost circled back around to being a good one. That‚Äôs all to say that I don‚Äôt really enjoy the cookie recipes that require you to beat sugars and butter until fluffy. This is one of them.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/pyenv-in-rstudio/index.html",
    "href": "posts/pyenv-in-rstudio/index.html",
    "title": "Pyenv in RStudio",
    "section": "",
    "text": "I am a chronic destroyer of Python environments. My favorite tools right now are pyenv and pyenv-virtualenv, which keep me mostly out of trouble, but there is some extra legwork to get them set up with RStudio IDE.\nI usually start by checking what Python is being used by going to the RStudio console and running:\nreticulate::py_config()\nThis gives me:\npython:         /usr/bin/python3\nlibpython:      /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/config-3.8-darwin/libpython3.8.dylib\npythonhome:     /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8:/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8\nversion:        3.8.9 (default, Aug  3 2021, 19:21:54)  [Clang 13.0.0 (clang-1300.0.29.3)]\nnumpy:           [NOT FOUND]\nwhich is nice, since at least some sort of Python is THERE, but I want to follow the Golden Rule: don‚Äôt touch the system Python.\nTime to do some environment gymnastics. My first move is to allow pyenv to share its environment, which is required to use envs with reticulate.\nenv PYTHON_CONFIGURE_OPTS=\"--enable-shared\" pyenv install 3.9.4\npyenv virtualenv 3.9.4 newenv\n\n\n\n\n\n\nNote\n\n\n\nIf you skip this step, you will get an error similar to:\nError: '/Users/isabelzimmerman/.pyenv/versions/3.9.11/envs/pydemo/bin/python' was not \nbuilt with a shared library. reticulate can only bind to copies of Python \nbuilt with '--enable-shared'.\n\n\nNext, we‚Äôre going to restart the R session, and try to just add this new Python environment in there. The key here is RESTART R SESSION (on macOS, this is commmand+shift+0) and then DON‚ÄôT TRY TO RUN ANYTHING IN PYTHON BEFORE THESE NEXT FEW COMMANDS. If you ‚Äújust try to run one thing to see if {insert ridiculous Python MultiEnvironmentVerse of Madness strategy} worked,‚Äù you will end up in an endless loop of setting environment variables that are ignored since they are already initialized (this sentence is hard-won for me). Let‚Äôs try to use reticulate to set Python first.\nreticulate::use_python('/Users/isabelzimmerman/.pyenv/versions/3.9.4/envs/newenv/bin/python', required = TRUE)\ngives us:\nWarning message:\nThe request to `use_python(\"/Users/isabelzimmerman/.pyenv/versions/3.9.11/envs/newenv/bin/python\")` \nwill be ignored because the environment variable RETICULATE_PYTHON is set to \"/usr/bin/python3\" \nThis is a helpful warning that tells us we need to set up the RETICULATE_PYTHON environment variable. We can do this by:\nSys.setenv(RETICULATE_PYTHON='/Users/isabelzimmerman/.pyenv/versions/3.9.4/envs/newenv/bin/python')\nand then check to see if it worked since we have pretty strong trust issues.\nSys.getenv(\"RETICULATE_PYTHON\")\n[1] \"/Users/isabelzimmerman/.pyenv/versions/3.9.4/envs/newenv/bin/python\"\nPromising! Let‚Äôs check out the reticulate::py_config() now.\nreticulate::py_config()\npython:         /Users/isabelzimmerman/.pyenv/versions/3.9.4/envs/newenv/bin/python\nlibpython:      /Users/isabelzimmerman/.pyenv/versions/3.9.4/lib/libpython3.9.dylib\npythonhome:     /Users/isabelzimmerman/.pyenv/versions/3.9.4/envs/newenv:/Users/isabelzimmerman/.pyenv/versions/3.9.4/envs/newenv\nversion:        3.9.4 (default, May 31 2022, 09:32:34)  [Clang 13.0.0 (clang-1300.0.29.3)]\nnumpy:          /Users/isabelzimmerman/.pyenv/versions/3.9.4/envs/newenv/lib/python3.9/site-packages/numpy\nnumpy_version:  1.23.1\n\nNOTE: Python version was forced by RETICULATE_PYTHON\n\n\n\n\n\n\nNote\n\n\n\nIf this doesn‚Äôt work, I would tell you to go digging for your RETICULATE_PYTHON environment variable. Some places to check out:\nvi ~/.Renviron\nshould open a file that has environment variables, specifically our elusive RETICULATE_PYTHON. We can update that file to look like:\nexport RETICULATE_PYTHON=/Users/isabelzimmerman/.pyenv/versions/3.9.4/envs/newenv/bin/python\n\n\nIT WORKED! Time to run all the Python I can. I‚Äôm fully open to better workflows if you have them, but for now, this gets me on my way when I feel the despair of another broken Python environment.\nAlso, S/O to Firas Sadiyah‚Äôs blog post on pyenv + RStudio which has helped me many a time to get on the right track!"
  },
  {
    "objectID": "posts/state-of-vetiver-2022/index.html",
    "href": "posts/state-of-vetiver-2022/index.html",
    "title": "A year in review: vetiver",
    "section": "",
    "text": "It has been almost a year of vetiver! Vetiver for Python (0.1.8) and R (0.1.8) seeks to provide fluent tooling to version, share, deploy, and monitor a trained model."
  },
  {
    "objectID": "posts/state-of-vetiver-2022/index.html#the-journey",
    "href": "posts/state-of-vetiver-2022/index.html#the-journey",
    "title": "A year in review: vetiver",
    "section": "The journey",
    "text": "The journey\nRecently, the team dedicated a month to reading about the world of MLOps, as it is today. We all read the fantastic book Designing Machine Learning Systems by Chip Huyen and split up numerous other academic articles and web content between ourselves. The world of machine learning operations moves fast, and we wanted to ensure the choices we had made early on (eg, focusing on versioning, deploying, and monitoring) would serve data practitioners best.\nAfter reading many different definitions of MLOps, the one we found most useful is: ‚Äúa set of practices to deploy and maintain machine learning models in production reliably and efficiently.‚Äù While not every MLOps practice is applicable at scale for every team, these best practices can elevate any size of project.\nMLOps applies the same basic principles as DevOps (development operations) in a specialized machine learning context. One common point of tension in MLOps is that data science is highly experimental and iterative compared to general purpose software delivery, but deploying to a production environment still requires reliable software engineering delivery practices. To help ease this pain point, APIs are commonly used to deploy models due to their stability and simplicity. APIs can be tested, and they act nearly identically in every architecture. This allows for software engineering practices to be applied to APIs. They have a straightforward architecture to configure and update, giving data scientists agility to retrain and update models as needed.\nTools labeled as ‚ÄúMLOps frameworks‚Äù have a broad scope. Tasks generally fall into one of a few different categories:\n\nOrchestration or pipelines\nExperiment tracking\nModel versioning\nModel deployment\nModel monitoring\n\nVetiver spans a few of these tasks, but is not a tool built for orchestration or experiment tracking. Rather, Vetiver focuses on the practices of versioning, deploying, and monitoring and will continue building support for these tasks."
  },
  {
    "objectID": "posts/state-of-vetiver-2022/index.html#where-vetiver-is-now",
    "href": "posts/state-of-vetiver-2022/index.html#where-vetiver-is-now",
    "title": "A year in review: vetiver",
    "section": "Where vetiver is now",
    "text": "Where vetiver is now\nVetiver also leverages the versioning and sharing capabilities of the pins package in Python and R. This package brings a straightforward way to read and write objects to different locations and allows for certain data types (namely csv and arrow files) to be passed between the Python and R language fluently.\nWithin vetiver itself, the use of VetiverModel, VetiverAPI, and monitoring helper functions gives practitioners lightweight support to bring their models to many different locations via one line deployment functions (for Posit Connect) or Dockerfile generation (for numerous on-prem or public cloud locations). These objects are able to be extended to support more advanced use cases. Vetiver is able to quickly prototype REST APIs, and then scale these prototypes safely."
  },
  {
    "objectID": "posts/state-of-vetiver-2022/index.html#where-vetiver-is-going",
    "href": "posts/state-of-vetiver-2022/index.html#where-vetiver-is-going",
    "title": "A year in review: vetiver",
    "section": "Where vetiver is going",
    "text": "Where vetiver is going\nFluent monitoring practices is crucial for a robust deployment. While the CI/CD in monitoring can be infrastructure dependent, it is important to close the loop between model prediction, monitoring, and retraining. Feedback loops are a place where bias in models can aggregate undetected. Any continuous monitoring support necessitates careful thought on how to uncover model (un)fairness.\nThe composability of vetiver with other projects, such as MLFlow or Metaflow, is needed to allow practitioners to build an MLOps framework that is flexible and meets the need of their team. Composability in this sense also includes platform agnosticism for public clouds such as Amazon Web Services, Azure, and Google Cloud Platform. Currently, generic Dockerfiles exist that can be hosted on these platforms, but extended documentation is needed for specific workflows."
  },
  {
    "objectID": "posts/state-of-vetiver-2022/index.html#where-vetiver-is-not-going",
    "href": "posts/state-of-vetiver-2022/index.html#where-vetiver-is-not-going",
    "title": "A year in review: vetiver",
    "section": "Where vetiver is not going",
    "text": "Where vetiver is not going\nDAG creation is currently out of scope of vetiver. If this is to be supported later on, it would likely end up in a new tool for maximum flexibility.\nSupport for automatic creation of model registries is not currently short-term plan. However, using pins and Quarto together can create a beautiful document to track your deployed objects. If you‚Äôre interested in this topic, this demo shows how to use pins + Quarto to track your models.\nIn all, we have learned so much from you all this year, and look forward to another year of helping data scientists bring models into production!"
  },
  {
    "objectID": "posts/multiple-models-vetiver/index.html",
    "href": "posts/multiple-models-vetiver/index.html",
    "title": "Multiple models on one API",
    "section": "",
    "text": "As someone who identifies as a Chicago Bears fan for life, I have some strong superstitions rituals on gameday. In a completely hypothetical example, I might have one model that I feel must be used when the Bears have a home game, and a different model for any other day. (They always win when I use this model to write blog posts on game days, and I fear a replay of the 2016 season when our win-loss ratio was 3-13 if I change my ways.)\nOftentimes you need to deploy more than one model. Even if the fate of your favorite NFL team isn‚Äôt depending upon your models‚Äô locations, it‚Äôs important to understand where these models should be living. tldr;\n\nInput data is the same -&gt; use one API\nInput data is not the same -&gt; use multiple APIs\n\n(These are not definitive rules. A lot of this is dependent upon architecture, how your deployment is set up, what works best for your organization, etc. Also, vetiver allows you to break both of these rules!)\nIf your models are unrelated and the input data is different, you probably want to put them on different APIs. However, if the input data is the same for multiple different models, it might make sense to deploy them on the same API, but at different endpoints.\nFor our Chicago train ridership data, we ALWAYS want to predict ridership from the same parameters every time. However, if the data indicates it was a home game for the Chicago Bears, we want to use our lucky model.\nLet‚Äôs start by loading some data and use tidymodels to put all our preprocessing in one recipe.\n\nlibrary(tidymodels)\ndata(Chicago)\n\nchicago_small &lt;- Chicago %&gt;% slice(1:730) # two years of data\n\nchicago_rec &lt;-\n  recipe(ridership ~ ., data = Chicago) %&gt;%\n  step_date(date) %&gt;%\n  step_holiday(date, keep_original_cols = FALSE) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  step_pca(all_of(stations), num_comp = 4)\n\nWe can then use tidymodels to make multiple different models quickly. Notice that I am actually putting my preprocessing recipe in the recipe with my model. This is intentional, and best practice for modeling.\n\n\n\n\n\n\nImportant\n\n\n\nFeature engineering is part of your model workflow. You should be packaging this up with your model and deploying it as part of your model workflow. In Python, this involves using something like scikit learn Pipelines. In R, this involves using something like tidymodels workflows.\n\n\n\nBuild a few models for home games (support vectors for extra support at home):\n\ntree_model &lt;-\n  svm_linear() %&gt;%\n  set_engine(\"LiblineaR\") %&gt;%\n  set_mode(\"regression\")\n\nhome_game_model &lt;-\n  workflow(chicago_rec, tree_model) %&gt;%\n  fit(chicago_small)\n\nAnd not home games:\n\nlinear_model &lt;-\n  linear_reg() %&gt;%\n  set_engine(\"lm\")\n\nnot_home_game_model &lt;-\n  workflow(chicago_rec, tree_model) %&gt;% \n  fit(chicago_small)\n\nNext, I will make some deployable model objects, or vetiver_model, with the vetiver package.\n\nlibrary(vetiver)\n\nhome &lt;- vetiver_model(home_game_model, \"home\")\nnot_home &lt;- vetiver_model(not_home_game_model, \"not_home\")\n\nNormally we would be iterating through models and possibly versioning them as well, but let‚Äôs skip that step and go right to deployment.\nI‚Äôll deploy these two models on ONE plumber API by using a combination of vetiver_pr_post() and vetiver_pr_docs() along with the path argument.\n\nlibrary(plumber)\n\npr() %&gt;% \n  vetiver_pr_post(home, path = \"/home_game\") %&gt;% \n  vetiver_pr_docs(home) %&gt;% \n  vetiver_pr_post(not_home, path = \"/not_home_game\") %&gt;% \n  vetiver_pr_docs(not_home) %&gt;% \n  pr_run()\n\nNow, to make predictions, I will route data to each endpoint respectively. To do this on your own computer, you will have to run the above commands as background job (or deploy it with docker.)\nWe can make predictions at each endpoint with data. Of course this can be as complex or simple as you desire, but here‚Äôs the meat-and-potatoes of it.\n\nhome_endpoint &lt;- vetiver_endpoint('http://127.0.0.1:5331/home_game')\n\nhome_data &lt;- Chicago %&gt;%\n  filter(Bears_Home == 1) %&gt;%\n  tail(5)\n\npredict(home_endpoint, home_data)\n\n\nnot_home_endpoint &lt;- vetiver_endpoint('http://127.0.0.1:5331/not_home_game')\n\nnot_home_data &lt;- Chicago %&gt;%\n  filter(Bears_Home == 0) %&gt;%\n  tail(5)\n\npredict(not_home_endpoint, not_home_data)\n\nWe have now created an API with multiple models at various endpoints, and successfully interacted with them! This is a great start to making more complex MLOps workflows with vetiver."
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "What I‚Äôm reading",
    "section": "",
    "text": "The data in this report automatically gets automatically updated monthly, so this page should be relatively up-to-date on all the books I‚Äôve reviewed on either Goodreads or Fable. This is not comprehensive of all the books I‚Äôve read, but only what I‚Äôve reviewed since I started tracking my reading in fall 2023. I‚Äôm always happy to give a recommendation, especially if you‚Äôre also a sci-fi or fantasy lover!\nThis page itself is a scratchpad/living document that I continue to update as I am curious about my own reading habits. I expect it to become prettier over time, but I thought it would be fun to share, even in a slightly WIP state! Disclaimer: Positron Assistant helped me make some of these plots.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMost read authors\n\n\n\n\n\n\n\n\n\n\n\nMy reviews vs.¬†community\nI‚Äôm always curious if my opinions line up with the community as a whole.\n\n\n                            \n                                            \n\n\n\n\nFive star books\nThese are all books that I have rated as five star reads. I‚Äôve sorted them here by highest-rated by the community to hopefully help you find your new favorite book!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does the community think of my five star ratings?\n\n\nCover\nTitle\nAuthor\nPrimary genre\nCommunity rating\n\n\n\n\n\nDemon in White\nChristopher Ruocchio\nSci-Fi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nMorning Star\nPierce Brown\nSci-Fi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nSunrise on the Reaping\nSuzanne Collins\nSci-Fi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nKingdoms of Death\nChristopher Ruocchio\nSci-Fi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nNo Visible Bruises: What We Don‚Äôt Know About Domestic Violence Can Kill Us\nRachel Louise Snyder\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nLet This Radicalize You\nKelly Hayes\nBiography\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nAshes of Man\nChristopher Ruocchio\nSci-Fi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nEducated\nTara Westover\nBiography\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Long Game\nRachel Reid\nLGBTQ+\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCatching Fire\nSuzanne Collins\nSci-Fi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Hunger Games\nSuzanne Collins\nSci-Fi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Man Called Ove\nFredrik Backman\nLiterary Fiction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIsles of the Emberdark\nBrandon Sanderson\nFantasy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn Tyranny\nTimothy Snyder\nHistory\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Song of Achilles\nMadeline Miller\nClassics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPages and books per month\nA running tally of how much I‚Äôm reading per month! I‚Äôm particularly curious about the difference between number of books versus number of pages."
  },
  {
    "objectID": "posts/audiobook-calculator/index.html",
    "href": "posts/audiobook-calculator/index.html",
    "title": "Reading Time Calculator",
    "section": "",
    "text": "&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n  &lt;meta charset=\"UTF-8\"&gt;\n  &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n  &lt;style&gt;\n    body {\n      font-family: Arial, sans-serif;\n      margin: 20px;\n    }\n    .calculator {\n      max-width: 450px;\n      margin: 0 auto;\n      padding: 40px;\n      border: 4px solid rgb(233, 150, 205);\n      border-radius: 8px;\n      box-shadow: 0 2px 4px rgba(165, 37, 122, 0.1);\n    }\n    .calculator input {\n      width: 100%;\n      margin-bottom: 16px;\n      padding: 8px;\n      border: 1px solid #ccc;\n      border-radius: 4px;\n    }\n    .calculator button {\n      width: 100%;\n      padding: 10px;\n      background-color: #CED5DA;\n      color:rgb(0, 0, 0);\n      border: none;\n      border-radius: 4px;\n      cursor: pointer;\n    }\n    .calculator button:hover {\n      background-color:rgb(233, 150, 205);\n    }\n  &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;div class=\"calculator\"&gt;\n    &lt;label for=\"hours\"&gt;Book Length (Hours)&lt;/label&gt;\n    &lt;input type=\"number\" id=\"hours\" value=\"0\" min=\"0\"&gt;\n\n    &lt;label for=\"minutes\"&gt;Book Length (Minutes)&lt;/label&gt;\n    &lt;input type=\"number\" id=\"minutes\" value=\"0\" min=\"0\"&gt;\n\n    &lt;label for=\"seconds\"&gt;Book Length (Seconds)&lt;/label&gt;\n    &lt;input type=\"number\" id=\"seconds\" value=\"0\" min=\"0\"&gt;\n\n    &lt;label for=\"speed\"&gt;Reading Speed&lt;/label&gt;\n    &lt;input type=\"number\" id=\"speed\" value=\"1.0\" min=\"0.1\" step=\"0.1\"&gt;\n\n    &lt;button onclick=\"calculateTime()\"&gt;Go&lt;/button&gt;\n\n    &lt;div class=\"result\" id=\"result\"&gt;&lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;script&gt;\n    function calculateTime() {\n      // Get inputs\n      const hours = parseInt(document.getElementById('hours').value) || 0;\n      const minutes = parseInt(document.getElementById('minutes').value) || 0;\n      const seconds = parseInt(document.getElementById('seconds').value) || 0;\n      const speed = parseFloat(document.getElementById('speed').value) || 1.0;\n\n      // Convert book length to total seconds\n      const totalSeconds = (hours * 3600) + (minutes * 60) + seconds;\n\n      // Calculate adjusted time\n      const adjustedSeconds = totalSeconds / speed;\n\n      // Convert back to hours, minutes, and seconds\n      const resultHours = Math.floor(adjustedSeconds / 3600);\n      const resultMinutes = Math.floor((adjustedSeconds % 3600) / 60);\n      const resultSeconds = Math.round(adjustedSeconds % 60);\n\n      // Display result\n      const resultElement = document.getElementById('result');\n      resultElement.textContent = `Adjusted Reading Time: ${resultHours} hours, ${resultMinutes} minutes, ${resultSeconds} seconds.`;\n    }\n  &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\n\n\n\n  \n  \n  \n\n\n  \n    Book Length (Hours)\n    \n\n    Book Length (Minutes)\n    \n\n    Book Length (Seconds)\n    \n\n    Reading Speed\n    \n\n    Go"
  },
  {
    "objectID": "posts/learn-mlops/index.html",
    "href": "posts/learn-mlops/index.html",
    "title": "So, you want to learn about MLOps",
    "section": "",
    "text": "If you‚Äôre interested in learning more about MLOps, I‚Äôve culminated a list of some of my favorite resources to learn about MLOps as a whole as well as specific MLOps practices."
  },
  {
    "objectID": "posts/learn-mlops/index.html#starting-out",
    "href": "posts/learn-mlops/index.html#starting-out",
    "title": "So, you want to learn about MLOps",
    "section": "Starting out",
    "text": "Starting out\nIf you‚Äôre wanting to invest a books-length amount of time into learning about MLOps, Designing Machine Learning Systems by Chip Huyen is my number one recommendation. This book has information for the whole machine learning lifecycle from building a model to building a system for that model to live in. If you‚Äôre mostly interested in the MLOps parts, I recommend starting with chapters 2, 7, 8, 9, and 11.\nMLOps Principles is a culmination of a lot of the most-cited MLOps literature, so if you‚Äôre wanting a starting point with lots of side quest opportunities, this is a good place to start. I particularly liked the idea of ‚ÄúContinuous X‚Äù; this extends the idea of CI/CD (continuous integration/continuous delivery) to include Continuous Training and Continuous Delivery. It highlights out an important piece of MLOps: it‚Äôs never finished.\nIf you‚Äôre interested in learning what ‚Äúreal‚Äù MLOps people do, ‚ÄúOperationalizing Machine Learning: An Interview Study‚Äù by Shreya Shankar et al is the article to read. This piece feels foundational to me. There is important discussion about the fundamental differences between building models and building traditional software, why not every model gets deployed, and much much more. It‚Äôs a digestable paper, and I highly recommend reading this one all the way through.\nAnd, I can‚Äôt write this without giving a personal plug: I‚Äôve given a number of MLOps talks, which you can view at the ‚ÄúTalks‚Äù tab on this page! A good ‚Äústarting point‚Äù talk of mine is Demystifying MLOps, which I presented at rstudio::conf(2022)."
  },
  {
    "objectID": "posts/learn-mlops/index.html#special-topics",
    "href": "posts/learn-mlops/index.html#special-topics",
    "title": "So, you want to learn about MLOps",
    "section": "Special topics",
    "text": "Special topics\nOnce you have a bit of MLOps context in your mind, there are lots of avenues to explore! If you‚Äôre interested in the pain points of How ML Breaks: A Decade of Outages for One Large ML Pipeline by Papasian and Underwood. This video walks through all the ways the ML pipeline at Google has broken; breakages are categorized in numerous ways, but one particularly interesting one is ML failures/Non-ML failures. Spoiler alert: most failures were not machine learning failures.\nWith a model in production, you‚Äôll want to monitor it. Reliable Maintenance of Machine Learning Models by Julia Silge is a great overview of why you should be monitoring models. This talk outlines what ‚Äúperforming well‚Äù means to different stakeholders of model systems and explains the multiple types of drift that can happen once your model is out in the wild.\nIf you have implemented MLOps practices, ‚ÄúThe ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction‚Äù by E.Breck et al.¬†2017 is a must read. It shows and rates a practical progression of machine learning systems, which is often not adding versioning systems -&gt; adding model deployment -&gt; starting to monitor models; rather, it usually follows the pattern of systems are not implemented -&gt; systems are implemented manually -&gt; systems are run automatically.\nEvent streaming won‚Äôt be a required skill for all people who are using MLOps practices. However, if you want to learn about it, Gently Down the Stream by Mitch Seymour is a children‚Äôs book which also happens to be the best explanation of Kafka I have ever read. Plus it is an adorable, digestible, maybe 5-minute read.\nFinally‚Äì if you‚Äôve devoured this list and are still hungry for more, MLOps guide by Chip Huyen is a secondary curated list of MLOps content that I have gone through and absolutely adore."
  },
  {
    "objectID": "posts/blind-date-with-a-data-talk/index.html",
    "href": "posts/blind-date-with-a-data-talk/index.html",
    "title": "Blind date with a data talk",
    "section": "",
    "text": "A ‚Äúblind date with a book‚Äù is where a book gets wrapped up in nondescript paper, and all the intro you get is a vague bullet point or two. You can‚Äôt judge a book by it‚Äôs cover, since, well, you can‚Äôt see it. You have to trust the synopsis and enjoy the adventure.\nI got the idea to make a ‚Äúblind date‚Äù with a data science talk after starting to curate a list of my favorite talks for a friend. Unrelated, I have been itching to play around with the shinylive project to put some interactive things on my blog. Why not put these two together?\n#| standalone: true\n#| viewerHeight: 300\n\nimport random\nfrom shiny import reactive, render\nfrom shiny.express import input, ui\nfrom pathlib import Path\n\ncss_file = Path(__file__).parent / \"styles.css\"\n\ndates = {\n    \"humor\": [\n        {\n            \"name\": \"Joel Grus: I don't like notebooks\",\n            \"link\": \"https://www.youtube.com/watch?v=7jiPeIFXb6U\"\n        }, # joel grus i dont like notebooks\n        {\n            \"link\":\"https://www.youtube.com/watch?v=hfqjyeA_z7s&list=PL9HYL-VRX0oRFZslRGHwHuwea7SvAATHp&index=40\",\n            \"name\": \"Hadley Wickham: It's a Great Time to be an R Package Developer!\"}, # hadley slide karaoke\n        {\n            \"link\":\"https://www.youtube.com/watch?v=C9OlkY87vf8&list=UULFYhY67dMuJ_w_9_cfkaruWQ&index=8\",\n            \"name\": \"Vicki Boykis: An ML Fairytale\"\n        }, # vicki boykis ml fairy tale\n        {\n            \"link\": \"https://www.youtube.com/watch?v=D48NQTNg19s&list=UULFYhY67dMuJ_w_9_cfkaruWQ&index=40\",\n            \"name\":\"Jacqueline Nolis: Alaska challenged my preconceived notions of storing sunset data\"\n        }, # jacqueline nolis time\n        {\n            \"link\": \"https://www.youtube.com/watch?v=Pm9C-Cz4bXE&t\",\n            \"name\": \"JD Long: I'd have written a shorter solution but I didn't have the time\"\n        } # jd long shorter \n        ],\n    \"story\": [\n        {\n            \"link\": \"https://youtu.be/Pa1PNfoOp-I?si=rF3udat0nV9n04w6\",\n            \"name\": \"JD Long: It's Abstractions All the Way Down...\"\n        }, # jd long abstractions\n        {\n            \"link\": \"https://youtu.be/DVQJ39_9L0U?si=PbsjmQR96rrz7scy\",\n            \"name\": \"Kari Jordan: Black Hair and Data Science Have More in Common Than You Think\"\n        }, # kari jordan black hair\n        {\n            \"link\": \"https://www.youtube.com/watch?v=g1ib43q3uXQ\",\n            \"name\": \"Felienne Hermans: How to teach programming (and other things)?\"\n        }, #  felienne hermans excel\n        {\n            \"link\": \"https://www.youtube.com/watch?v=s3WbEfoxRjs\",\n            \"name\": \"Sophie Watson: What they didn‚Äôt teach you in Grad School\"}, # sophie watson what they didnt teach u\n        ],\n    \"nerd\": [\n        {\n            \"link\": \"https://www.youtube.com/watch?v=sYliwvml9Es\",\n            \"name\": \"Jeremy Howard: A hacker's guide to open source LLMs\"\n        }, # jeremy howard llm\n        {\n            \"link\": \"https://www.youtube.com/watch?v=-YEUFGFHWgQ\",\n            \"name\": \"Calvin Hendryx-Parker: Bootstrapping Your Local Python Environment\"}, # calvin hendryx-parker bootstrapping python\n        {\n            \"link\": \"https://www.youtube.com/watch?v=7nNB__jK9AY\",\n            \"name\": \"Alison Presmanes Hill: The Happiest Notebooks on Earth\"\n        }, # \n        {\n            \"link\": \"https://www.youtube.com/watch?v=EvQUVzDJRJ8&list=UULFYhY67dMuJ_w_9_cfkaruWQ&index=42\",\n            \"name\": \"Chelsea Parlett-Pelleriti: Why Are You The Way That You Are: Sklearn Quirks\"\n        } # chelsea parlett scikit\n        ],\n    \"reveal\": [\n        {\n            \"link\": \"https://www.youtube.com/watch?v=qKfkCY7cmBQ\",\n            \"name\": \"Peter Wang: Programming for everyone (or the next 100 million Pythonistas)\"}, # peter wang pyright\n        {\n            \"link\": \"https://www.youtube.com/watch?v=HpqLXB_TnpI\",\n            \"name\": \"Joe Cheng: The Past and Future of Shiny\"\n        }, # joe cheng shiny py \n        {\n            \"link\": \"https://www.youtube.com/watch?v=p7Hxu4coDl8\", \n            \"name\": \"Mine √áetinkaya-Rundel & Julia Stewart Lowndes: Hello Quarto: Share, Collaborate, Teach, Reimagine\"\n        }, # mine and julia hello quarto\n    ],\n    \"small\": [\n        {\n            \"link\": \"https://www.destroyallsoftware.com/talks/wat\",\n            \"name\": \"Destroy all software: WAT\"\n        }, # wat\n        {\n            \"link\": \"https://youtu.be/V3XdLVAwmX0?si=5gPV2bXsgURLqCYG\",\n            \"name\": \"Libby Heeren: Why You Should Stop Networking and Start Making Friends\"\n        }, # libby heeren stop networking\n        {\n            \"link\": \"https://youtu.be/ZDK5DZOgHD8?si=3q1q0Zo0z-Oet_Bb&t=1150\",\n            \"name\": \"Katy Huff: I do (automate things)\"\n        }, # katy huff wedding website\n        {\n            \"link\": \"https://www.youtube.com/watch?v=ES1LTlnpLMk&list=UULFYhY67dMuJ_w_9_cfkaruWQ&index=41\",\n            \"name\": \"Jenny Bryan: How to name files\"\n        } # jenny bryan file name\n        ]\n}\n\nui.include_css(css_file)\n\nui.input_radio_buttons(  \n    \"radio\",  \n    \"I want a data science talk with:\",  \n    {\n        \"humor\": \"Lots of humor\", \n        \"story\": \"A good story\", \n        \"nerd\": \"Maximum nerdiness\",\n        \"reveal\": \"An exciting new tech for the time\",\n        \"small\": \"Something bite-sized\"\n    },  \n)\nui.input_action_button(\"action_button\", \"Find me a talk üîÆ\")  \n\n@render.express()\n@reactive.event(input.action_button)\ndef _():\n    k = input.radio()\n    rec_talk = random.choice(dates[k])\n    ui.HTML(f'&lt;p&gt;&lt;/p&gt;&lt;a href={rec_talk[\"link\"]} target=\"_blank\" color=\"purple\"&gt;{rec_talk[\"name\"]}&lt;/a&gt;')\n\n## file: styles.css\nbody {\n    background-color: #FDFCFC;\n    color: #404040;\n  }\na {\n  color: #404040;\n}\nShoutout to Madison Yonash for asking about new data science talks and effectively nerd sniping me into spending an evening building this."
  },
  {
    "objectID": "posts/gha-making-gha/index.html",
    "href": "posts/gha-making-gha/index.html",
    "title": "Github Actions all the way down",
    "section": "",
    "text": "The lore\nI spent a while exploring how Python packages are made. It‚Äôs the wild west out there. There‚Äôs too many setup files, testing frameworks, no agreed-upon directory structure, and generally enough information to make your head spin. Luckily, you can use tools like cookiecutter to quickly get started (once you figure out which cookiecutter you want to use). For my use case, I wanted to make a simple cookiecutter, but wanted to run some tests to make sure my configuration of files did all the things I expected it to do.\nThe workflow was:\n1. Edit my `cookiecutter`. \n2. Run my cookiecutter to make a new package. \n3. Push my new package to Github. \n4. Run Github Actions. \n5. Realize I messed up in step 1.\nBut what if, Github Actions could use the cookie cutter repo to create ANOTHER repo to automatically generate your package? And what if that freshly cookie-cut repo would run its own Github Actions to ensure everything was properly set up? Useless? Most likely. But, I was going to do it anyway.\n\n\nSee it in (Github) action\nGitHub Action that pushes to a different repo.\nGitHub Actions in receiving repo that runs once a push has been made.\n\n\nSet up\nYou will need two different repos, a sending repo and receiving repo. In my case, the sending repo generates a cookiecutter template for a Python package, and then the receiving repo is the output package. You don‚Äôt have to be familiar with cookiecutter templates to understand this gist, but they‚Äôre worth a quick click if organization brings you joy.\nBefore running your chain reaction of actions, a few pieces need to be set up. The receiving repo cannot be empty, so I just initialized it with an empty README.md. The next part is probably the most difficult if you‚Äôre unfamiliar, which is generating an SSH key, click here for instructions. Once you‚Äôve gone into the terminal and generated the key, add the PUBLIC KEY to your SENDING repo and the PRIVATE KEY to your RECEIVING repo.\n\n\nReady, ACTION!\nThe GitHub Action that sets off this package creation is below, or you can see the action in the repo, for context. Note: there are difference between the code below and the code in the repo, these changes were made for clarity and should not impact performance.\n# Creates cookie cutter and pushes to new repo\nname: Generate Cookiecutter\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n  workflow_dispatch:\n  \njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - uses: actions/setup-python@v2\n      - name: Install requirements\n        run: pip install cookiecutter\n      - name: Make cookiecutter with default inputs\n        run: cookiecutter --no-input .\n      - name: Send to receiving repo\n        uses: peaceiris/actions-gh-pages@v3\n        with:\n          deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }}\n          external_repository: isabelizimm/making-cookies\n          publish_branch: dev-branch\n          publish_dir: ./my_new_package\n          exclude_assets: ''\nReally, the most important part here is the Send to receiving repo step. You see here that your sender deploy key is being utilized and the external repository and branch is specified. This Action will normally exclude certain files (in particular, we need .github/ from the template) on push, which is we have to manually set exclude_assets: ' ' so all files are being pushed.\nOnce this action is running successfully, you will see a new branch, dev-branch, on the receiving repo, making-cookies. Congratulations! The hard part is finished.\nThe second set of actions is less magical. This template package has built-in GitHub Actions to jump-start running tests and building docs. This second set of actions runs on push to any branch named ‚Äúdev-*‚Äù, so they automatically start after the initial push to the branch from the sending repo.\nHopefully this helps (or at least gives a starting point) others who are also exploring the capabilites of CI! Feel free to contact me if you have any suggestions or struggles. :)\ncheers, iz"
  },
  {
    "objectID": "posts/abstracts/index.html",
    "href": "posts/abstracts/index.html",
    "title": "Abstract art",
    "section": "",
    "text": "Starting to write a new conference proposal (also called an abstract) is my least favorite part of any conference experience. I‚Äôll think I have a half-decent idea, but then I open up a blank document and feel like I should be able to articulate my thoughts in a surprising-yet-thought-provoking, high-tech-yet-accessible, gentle-yet-striking way. After a first draft, I‚Äôll reread my work and realize the most striking thing that occurred was I wrote a sentence that was 15 lines long, of which 6 lines are actually related to what I wanted to talk about and the rest are talking about the fact I‚Äôd like to talk about it. Also, I typed ‚Äúteh‚Äù instead of ‚Äúthe.‚Äù It‚Äôs scary! Conference committee members use these few words to evaluate your topic‚Äôs fit into the conference schedule, and it will likely be the only context attendees use to determine if they want to watch your session. The overall goal of an abstract is to 1) inform the reader what your topic is about and 2) give compelling reasons to invest time and mental bandwidth in your ideas. Abstracts are an art form, but also a skill you can build to make the whole experience feel a little bit easier.\nLike solving any good engineering problem, the first step is to read the documentation (even if you feel like you already know what should be done). The conference likely has a page or two about the kinds of talks they are interested in, the length of an abstract, and previous talks. Note if there is a maximum length; double check if that length is characters or words. Check out the topics of previously accepted submissions to do a sanity check if your work fits in with prior art. A little bit of reading here is worthwhile to make sure you are setting yourself up for success in the submission stage.\nIf you are considering is my idea worth a submission?‚Äì the answer is YES! You have unique ideas, experiences, and thought processes that people want to hear. Learning how you and your team approached and solved a difficult problem, why you chose one framework over another, or even well-researched hot-takes all make for great presentations. If a full length talk seems too overwhelming, many conferences have lightning talks that are 3-10 minutes long you can sign up for beforehand or during the conference itself. Lightning talks are lots of fun; Tracy Teal has put together a great guide on how to prepare lightning talks.\nStart with a list of what you hope to achieve in your talk. At the very least, have your main topic plus, why people should care, and what people will learn. This list will become your guiding light on what really matters when you are tempted to add feature lists to your abstract.\nDuring a panel discussion on How to Get a Paper Accepted at OOPSLA, Kent Beck‚Äôs advice was that a good abstract only needs four sentences:\n\nThe first [sentence] states the problem. The second states why the problem is a problem. The third is my startling sentence. The fourth states the implication of my startling sentence.\n\nMost of the abstracts I write follow some loose interpretation of this four-sentence rule. The first sentence in an abstract should clearly explain the pain point you are trying to resolve in your talk; the second sentence is why that pain point matters. The third sentence is where you explain how your work solves the problem, or otherwise is the main idea of the talk. Finally, the fourth sentence is where you get to envision a world where someone uses your third-sentence solution. What process has been improved, what friction has been alleviated, or what has otherwise improved in someone‚Äôs life due to your talk? Make an argument as to why your talk is worth attending.\nIn the case of longer abstracts, I usually end up with three-ish paragraphs.\n\nThe first paragraph is created using the four-sentence rule.\nThe second paragraph provides evidence of why your solution is the best choice.\nThe third and final paragraph describes what knowledge listeners should know when they walk away.\n\nThe evidence outlined in the second paragraph will probably become the main subpoints in a slide deck when you are presenting. Think of the reasons why you believe your main idea is correct. Maybe you‚Äôre writing about how framework X is the best one; this paragraph could mention that framework X supports many relevant data types, is able to be integrated into the cloud backend that your team uses, and has a great community that helps answer all your questions. The third paragraph should answer the question: What will I know at the end of this talk, that I don‚Äôt know now? Keep in mind that the average listener will not walk away as an advanced user of your solution, since they will probably only remember 10% of your talk by the time they get back home. They might, however, still remember why your solution is a good idea and when to use it. The last 1-2 sentences of the abstract can be a recap of the problem and solution outlined in the first paragraph.\nWith the structure done, reread your abstract once and check for misspelled words, clarify any confusing wording, and remove or explain jargon. Jargon for one industry may be common knowledge in another, but use your best judgment on what is reasonable; have a friend or colleague read through your work as a second opinion if you‚Äôre not sure!\nIt‚Äôs easy to fall into the trap where you want to write your whole talk out when you are given a healthy word count to do so. Do remember that a limit is just that, a limit. Clearly outline the points you are going to make and why they are important, but this is not the time to explain all the details. Readers will appreciate brevity, so long as you give enough information that they can assess the relevance of your talk to their interests. It may help to remember who you are writing this abstract for:\n\nIf I am sure to consider my audience ‚Äì first, an overworked program committee member, and second, a jetlagged and overstimulated conference attendee ‚Äì I am far more likely to explain things clearly and eschew jargon. (William Benton‚Äôs excellent blog post ‚ÄúConcrete advice about abstracts‚Äù)\n\nYou‚Äôll want to have thought about your talk content, but in the abstract submission phase, it is not necessary to have a script or slides or anything extra prepared until you hear if your abstract is accepted.\nHappy conference season, wishing you all accepted talks!"
  },
  {
    "objectID": "posts/making-a-quarto-blog/index.html",
    "href": "posts/making-a-quarto-blog/index.html",
    "title": "We‚Äôre making a blog (IDE agnostic edition)",
    "section": "",
    "text": "My partner, Val, has been asking to making a fancy Quarto blog. Today, we‚Äôre going through this step-by-step together. There‚Äôs a few different ways to do this, and the RStudio IDE has UI to make this a pretty slick process. However, I don‚Äôt spend much time in RStudio, so here‚Äôs my IDE agnostic workflow. You‚Äôll need:\n\nA GitHub account\nTo install Quarto here if you haven‚Äôt yet\nAccess to the command line (eg, ‚ÄúTerminal‚Äù on a Mac)\n\n\nStep 1: Make the repository\nGo to github.com and click on your profile image in the top right corner. In the dropdown, click on Your repositories and then click New to create a new repository.\n\nCreate a new repository in GitHub and name it your-github-username.github.io. This is a secret repository structure in GitHub to host a blog for your account! Next, open up the terminal and use the cd command to navigate to the folder you would like to store your blog in. Use the command git clone {repository-url} to copy the repository onto your computer, and cd your-github-username.github.io into your local version.\n\n\nStep 2: Create blog\nIn the folder for your repository, type quarto create project blog . into your CLI tool and press enter. Congratulations, you have built a blog! Use quarto preview to build and view it locally.\n\n\nStep 3: Update the blog\nThere‚Äôs a few places you will want to update before you publish your blog for the world to see. First, open up the file called index.qmd and add some information about yourself. I have a few sentences of introduction, my education, and how to get in contact with me. Next, open up the folder called posts/ and update it to remove the default posts. If you‚Äôd like to write a blog post of your own, the easiest way to organize the posts is by making each one into its own folder. Then, each folder can contain a .qmd, .md, .Rmd, or .ipynb file as a post and any relevant images.\nFor simple aesthetics, you can go to the _quarto.yml file and try out different bootstrap themes, or add some color to your navbar by adding a background field to your navbar\nwebsite:\n  title: \"my blog\"\n  description: \"my personal blog\"\n  navbar:\n    background: \"#ffccff\" # accepts color names like red, blue, or hex codes\n\n\nStep 4: Push to Github Pages\nWhile still in the folder of your blog, run quarto publish gh-pages. You will be prompted with some text like: Publish site to your-github-username.github.io? Accept this, and let it run.\n\n\nStep 5: Set the deployment branch to gh-pages\nBack to GitHub, view your repository. Go to the settings for the repo, and click on Pages. Then, change the Build and deploy from source to the gh-pages branch.\n\nIf you go back to the main page of your repository, you will see a tab Deployments underneath the repository description. The green checkmark means your blog has been successfully deployed! The url will be something like your-github-username.github.io\nGoing forward, each time you want to update your website, you can reuse the quarto publish gh-pages command."
  },
  {
    "objectID": "posts/plotnine-contest/index.html",
    "href": "posts/plotnine-contest/index.html",
    "title": "Plotnine plot contest",
    "section": "",
    "text": "Plotnine is running a plot contest! And it is not too late to enter, submissions close July 12!\nFor my submission1, I‚Äôll start with reading in cheese data from TidyTuesday, as well as a csv of longitudes and latitudes of different countries.\nimport geopandas\nimport geodatasets\n\nimport pandas as pd\nimport numpy as np\n\nfrom plotnine import *\n\ncheese = pd.read_csv(\n    \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-06-04/cheeses.csv\"\n)\ncountry_lat_long = pd.read_csv(\n    \"./world_country_and_usa_states_latitude_and_longitude_values.csv\"\n)[[\"latitude\", \"longitude\", \"country\"]]\nNext, some data manipulation to set us up with tidy data to plot.\ncheese['country']=cheese['country'].str.split(pat=', ')\ncheese['milk']= cheese['milk'].str.split(pat=', ')\ncheese = cheese.explode('country').explode('milk')\nmode_milk = cheese.groupby('country')['milk'].agg(pd.Series.mode).reset_index()\ncountry_value = cheese[\"country\"].str.split(pat=', ').explode().str.strip().value_counts().reset_index()\ncheese_plot = country_value.merge(country_lat_long, how = \"left\", on=\"country\")\ncheese_plot = cheese_plot.merge(mode_milk, how = \"left\", on = \"country\").explode('milk')\ntop_countries = cheese_plot.sort_values(\"count\", ascending=False).replace([\"United Kingdom\"], [\"U.K.\"]).head(5)\nWe also will use geopandas in order to generate the map itself.\ngeodatasets.fetch(\"naturalearth land\")\nworld_lowres = geopandas.read_file(\n    \"https://github.com/geopandas/geopandas/raw/v0.9.0/geopandas/datasets/naturalearth_lowres/naturalearth_lowres.shp\"\n)\nworld = geopandas.read_file(geodatasets.get_path(\"naturalearth land\"))\nFinally, let‚Äôs use plotnine to put it all together! The main pieces in play are four geoms_ and then some extra layers to make the plot more readable. A geom_map to generate the map, geom_point to place each circle depicting number and type of cheeses, and two geom_text elements for the country name and number of cheeses. The scale_size edits the size of the points in geom_point and scale_colour_brewer edits the colors of these points. Finally, the theme is a combination of theme_void and custom theme and guides elements.\n(\n    ggplot()\n    + geom_map(world_lowres, color=\"#474c53\", fill=\"#d0d0d0\", stroke=\"1\")\n    + geom_point(\n        data=cheese_plot,\n        mapping=aes(x=\"longitude\", y=\"latitude\", size=\"count\", color=\"milk\"),\n        alpha=0.8,\n    )\n    + geom_text(\n        top_countries,\n        aes(x=\"longitude\", y=\"latitude\", label=\"country\"),\n        fontweight=\"bold\",\n        size=16,\n    )\n    + geom_text(\n        top_countries,\n        aes(x=\"longitude\", y=\"latitude\", label=\"count\"),\n        nudge_y=-4,\n        size=16,\n    )\n    + labs(\n        title=\"Say Cheese: A World Tour\",\n        subtitle=\"This map shows the number of cheese types per country and the most common milk used. This is nacho average cheese map!\",\n        caption=\"Data from cheese.com via the TidyTuesday project\",\n        color=\"Most common milk type used\",\n    )\n    + theme_void()\n    + scale_size(range=(3, 60), guide=None)\n    + scale_colour_brewer(type=\"qual\", palette=\"Accent\")\n    + theme(\n        figure_size=(20, 12),\n        legend_text_legend=element_text(size=14),\n        legend_direction=\"vertical\",\n        legend_title=element_text(size=16),\n        legend_position=(0.15, 0.35),\n        plot_title=element_text(size=36, family=\"fantasy\"),\n        plot_subtitle=element_text(size=18),\n        plot_caption=element_text(size=16),\n    )\n    + guides(colour=guide_legend(override_aes={\"size\": 8, \"alpha\": 1}))\n)\n\n/Users/isabelzimmerman/.pyenv/versions/3.11.4/envs/docker/lib/python3.11/site-packages/plotnine/layer.py:364: PlotnineWarning: geom_point : Removed 11 rows containing missing values.\nIf you‚Äôre interested in entering the plot contest, but aren‚Äôt sure what data to use, here are a few options that look like a lot of fun:\nHope to see some of your plots!\ncheers, isabel"
  },
  {
    "objectID": "posts/plotnine-contest/index.html#footnotes",
    "href": "posts/plotnine-contest/index.html#footnotes",
    "title": "Plotnine plot contest",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTruthfully, I made this plot because I‚Äôm a data nerd with a weird idea of fun. Since I‚Äôm an employee of the company Posit (who is sponsoring this contest), I cannot actually enter. But you can! Prizes include fun swag, subscriptions to services to host your portfolio, and the priceless bragging rights of being a open source plotting champion üèÜ‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/ai-laundry/index.html",
    "href": "posts/ai-laundry/index.html",
    "title": "Let AI do your coding laundry",
    "section": "",
    "text": "As someone who is still early-ish career (graduating from my undergrad in 2021), I worry about building my skills in the world of rising AI usage. I want to sharpen my craft, but also accept this changing world and want to use AI as a tool to help me be productive now but also continue to build skills that aren‚Äôt reliant on AI.\nThere‚Äôs a joke out there that we want AI to do the laundry so we have more time making art, but now AI is making art and we have to do the laundry. I‚Äôm actively trying to shift my use of AI where I can spend more time focusing on skills I want to grow, and let AI do the coding laundry."
  },
  {
    "objectID": "posts/ai-laundry/index.html#footnotes",
    "href": "posts/ai-laundry/index.html#footnotes",
    "title": "Let AI do your coding laundry",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can generally assume the tool I am using is Claude code with the Sonnet 3.7 or Sonnet 4.5 models. But I‚Äôll use the language of ‚ÄúAI tools‚Äù loosely here since I‚Äôm talking about the tasks I am performing, not the specific method of getting results.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/image-explainability-pets/index.html",
    "href": "posts/image-explainability-pets/index.html",
    "title": "Cat detection with explainers",
    "section": "",
    "text": "The growing availability of big data has increased the benefits of using complex models, forcing data scientists to choose between accuracy and interpretability of a model‚Äôs output. Adding explainability methods to models that are not easily understood helps:\n\nEnsure transparency algorithmic decision-making\nIdentify potential bias in the training data\nGive a deeper understanding of the process being modeled\n\nThese methods are able to give insight into why your model generates specific outputs; explainability algorithms are especially useful in highly regulated industries (ie, pinpoint the attributes that caused someone to be denied/approved a home loan). We‚Äôll demonstrate an anchor explainer in this notebook to better understand why a generic image detection model is creating certain outputs using the open-source library alibi.\n\nimport os\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3, decode_predictions\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom alibi.explainers import AnchorImage\n\nTo begin, we load and format the photo into a machine readable array.\n\ndef image_pipeline(image_name):  \n\n    #Format custom picture\n    resized_image = load_img(image_name, target_size=(299, 299))\n    numpy_image = img_to_array(resized_image)\n    image_batch = np.expand_dims(numpy_image, axis=0)\n    \n    model = InceptionV3(weights='imagenet')\n    preds = model.predict(image_batch)\n    \n    #Display results\n    label = decode_predictions(preds, top=3)\n    plt.title(label[0])\n    plt.imshow(resized_image)\n\n\nimage_name = 'cat.jpg'\nimage_pipeline(image_name)\n\n\n\n\n\n\n\n\nThe predict function returns the predictions via Softmax, which mean that the prediction can be translated as the probability that the image falls into one of the categories given in training. Even though we see this as a cat, the model gives a 100% probability that the image shows a pitcher. There are a few things to note here.\n\nThis is a photo I took of my own cat, so I can confidently know that the model has never seen the picture before. To a human eye, it is (hopefully) pretty obvious that the image is of a cat. However, oftentimes training data, especially images, does not accurately reflect photos that are taken by you or I. For example, the training photos may all have centered images in good lighting with nothing in the background, which is unrealistic for everyday images.\nThis is a Softmax output. The model only gives us the names of its best guesses, not how strong the prediction is. This is a subtle differentiation. The model believes that pitcher is definitely the best guess, but it could only be 1% certain in this prediction. To solve this, we could add new outcome metrics, or tweak the model.\n\nBefore we do any of that though, it could be useful for us to better understand how the model is making this prediction. We‚Äôll use something called an explainer to dive deeper."
  },
  {
    "objectID": "posts/image-explainability-pets/index.html#pet-image-detection",
    "href": "posts/image-explainability-pets/index.html#pet-image-detection",
    "title": "Cat detection with explainers",
    "section": "",
    "text": "The growing availability of big data has increased the benefits of using complex models, forcing data scientists to choose between accuracy and interpretability of a model‚Äôs output. Adding explainability methods to models that are not easily understood helps:\n\nEnsure transparency algorithmic decision-making\nIdentify potential bias in the training data\nGive a deeper understanding of the process being modeled\n\nThese methods are able to give insight into why your model generates specific outputs; explainability algorithms are especially useful in highly regulated industries (ie, pinpoint the attributes that caused someone to be denied/approved a home loan). We‚Äôll demonstrate an anchor explainer in this notebook to better understand why a generic image detection model is creating certain outputs using the open-source library alibi.\n\nimport os\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3, decode_predictions\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom alibi.explainers import AnchorImage\n\nTo begin, we load and format the photo into a machine readable array.\n\ndef image_pipeline(image_name):  \n\n    #Format custom picture\n    resized_image = load_img(image_name, target_size=(299, 299))\n    numpy_image = img_to_array(resized_image)\n    image_batch = np.expand_dims(numpy_image, axis=0)\n    \n    model = InceptionV3(weights='imagenet')\n    preds = model.predict(image_batch)\n    \n    #Display results\n    label = decode_predictions(preds, top=3)\n    plt.title(label[0])\n    plt.imshow(resized_image)\n\n\nimage_name = 'cat.jpg'\nimage_pipeline(image_name)\n\n\n\n\n\n\n\n\nThe predict function returns the predictions via Softmax, which mean that the prediction can be translated as the probability that the image falls into one of the categories given in training. Even though we see this as a cat, the model gives a 100% probability that the image shows a pitcher. There are a few things to note here.\n\nThis is a photo I took of my own cat, so I can confidently know that the model has never seen the picture before. To a human eye, it is (hopefully) pretty obvious that the image is of a cat. However, oftentimes training data, especially images, does not accurately reflect photos that are taken by you or I. For example, the training photos may all have centered images in good lighting with nothing in the background, which is unrealistic for everyday images.\nThis is a Softmax output. The model only gives us the names of its best guesses, not how strong the prediction is. This is a subtle differentiation. The model believes that pitcher is definitely the best guess, but it could only be 1% certain in this prediction. To solve this, we could add new outcome metrics, or tweak the model.\n\nBefore we do any of that though, it could be useful for us to better understand how the model is making this prediction. We‚Äôll use something called an explainer to dive deeper."
  },
  {
    "objectID": "posts/image-explainability-pets/index.html#explainability",
    "href": "posts/image-explainability-pets/index.html#explainability",
    "title": "Cat detection with explainers",
    "section": "Explainability",
    "text": "Explainability\nWe will use a local version of the model to build an anchor explainer. This will show us what parts of the photo the model used in order to give the ‚Äúpitcher‚Äù prediction.\n\ndef explainability_pipeline(image_name): \n    \n    # format custom picture\n    resized_image = load_img(image_name, target_size=(299, 299))\n    numpy_image = img_to_array(resized_image)\n    image_batch = np.expand_dims(numpy_image, axis=0)\n    \n    # set hyperparameters\n    image_shape = (299, 299, 3)\n    segmentation_fn = 'slic'\n    kwargs = {'n_segments': 15, 'compactness': 20, 'sigma': .5}\n\n    # load model\n    model = InceptionV3(weights='imagenet')\n    \n    # explainer, from alibi library\n    explainer = AnchorImage(model, image_shape, segmentation_fn=segmentation_fn, \n                            segmentation_kwargs=kwargs, images_background=None)\n    \n    cat_image = image_batch[0]\n    explanation = explainer.explain(cat_image, threshold=.95, \\\n                                    p_sample=.5, tau=0.25)\n\n    plt.imshow(explanation.anchor)\n\n\nexplainability_pipeline(image_name)\n\nskimage.measure.label's indexing starts from 0. In future version it will start from 1. To disable this warning, explicitely set the `start_label` parameter to 1.\n\n\n\n\n\n\n\n\n\nIt looks as though the model uses primarily the background to create this prediction, so it comes as no surprise that the classification is wildly incorrect. With this information in mind, data scientists may decide to go back and create a more robust exploration of the data and model. It may be the case that the training data of cats has only solid backgrounds, different lighting, different color/hair length cat, or some other feature that caused this image to not be identified correctly.\nExplainers will not fix the model itself. However, they are useful tools for data scientists to build well-architected models by exposing bias in training data and giving transparency to black-box models."
  },
  {
    "objectID": "posts/image-explainability-pets/index.html#try-your-own-pets",
    "href": "posts/image-explainability-pets/index.html#try-your-own-pets",
    "title": "Cat detection with explainers",
    "section": "Try your own pets!",
    "text": "Try your own pets!\nEither download or git clone this demo repo to your local machine. Import your own photo to the data folder. Change your-image-name in the final cell (see below for example) to match your image‚Äôs name, and press run!\n\ncustom_image_name = '../data/your-image-name.jpg'\nimage_pipeline(custom_image_name)\nexplainability_pipeline(custom_image_name)"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "isabel zimmerman",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nFeb 10, 2025\n\n\n0 to portfolio website\n\n\nisabel zimmerman + valentina colorado\n\n\n\n\n\n\nSep 20, 2023\n\n\nThanks, I made it in quartodoc\n\n\n¬†\n\n\n\n\n\n\nDec 2, 2022\n\n\nPractical MLOps for better models\n\n\n¬†\n\n\n\n\n\n\nNov 10, 2022\n\n\nHolistic MLOps for better science\n\n\n¬†\n\n\n\n\n\n\nOct 3, 2022\n\n\nBuilding an MLOps strategy from the ground up\n\n\n¬†\n\n\n\n\n\n\nJul 25, 2022\n\n\nDemystifying MLOps\n\n\n¬†\n\n\n\n\n\n\nOct 3, 2021\n\n\nExplaining model explainability\n\n\n¬†\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "ide-quiz.html",
    "href": "ide-quiz.html",
    "title": "What is your IDE soulmate?",
    "section": "",
    "text": "Answer each question by selecting the option that best describes you.\n\n\n\n    \n    \n    \n\n\n    \n        \n            \n                How interactive is your coding experience?\n                \n                    \n                        \n                        A) I mostly run .py or .R scripts without needing notebooks or other interactive consoles.\n                    \n                    \n                        \n                        B) I experiment a lot with code, using a mix of scripts, notebooks, and a console.\n                    \n                    \n                        \n                        C) I solely use notebooks for my experiments.\n                    \n                \n            \n\n            \n                How do you explore data?\n                \n                    \n                        \n                        A) I have small or medium data that I would like to explore with an interface.\n                    \n                    \n                        \n                        B) I have small, medium, or large data that I would like to explore with an interface.\n                    \n                    \n                        \n                        C) I don‚Äôt need to view my data in an explorer format.\n                    \n                \n            \n\n            \n                Do you love virtual environments?\n                \n                    \n                        \n                        A) I might use multiple virtual environments, but don‚Äôt switch frequently.\n                    \n                    \n                        \n                        B) I use many virtual environments and want the to experiment with them at the same time. \n                    \n                    \n                        \n                        C) I use 0-1 virtual environments.\n                    \n                \n            \n\n            \n                Are you a data author?\n                \n                    \n                        \n                        A) I use authoring tools like Quarto, but do not need a rich environment.\n                    \n                    \n                        \n                        B) I use authoring tools like RMarkdown and Quarto and would like UI for common tasks.\n                    \n                    \n                        \n                        C) I am comfortable using Quarto on the command line or using other tools for authoring.\n                    \n                \n            \n\n            Get Results\n        \n\n        \n            Your Quiz Results:\n            \n            Take Quiz Again"
  },
  {
    "objectID": "talks/positconf2023/index.html",
    "href": "talks/positconf2023/index.html",
    "title": "Thanks, I made it in quartodoc",
    "section": "",
    "text": "When Python package developers create documentation, they typically must choose between mostly auto-generated docs or writing all the docs by hand. This is problematic since effective documentation has a mix of function references, high-level context, examples, and other content. Quartodoc is a new documentation system that automatically generates Python function references within Quarto websites. This talk will discuss pkgdown‚Äôs success in the R ecosystem and how those wins can be replicated in Python with quartodoc examples. Listeners will walk away knowing more about what makes documentation delightful (or painful), when to use quartodoc, and how to use this tool to make docs for a Python package.\nView the repository\n\n\n\nI had so much fun as posit::conf(2023)! The community here is ELECTRIC. Everyone is so engaged and interested and genuine (and nerdy). It feels like a show and tell with all your best friends. I had the pleasure of talking about my documentation journey with Python packages, and how the project quartodoc felt just right."
  },
  {
    "objectID": "talks/positconf2023/index.html#abstract",
    "href": "talks/positconf2023/index.html#abstract",
    "title": "Thanks, I made it in quartodoc",
    "section": "",
    "text": "When Python package developers create documentation, they typically must choose between mostly auto-generated docs or writing all the docs by hand. This is problematic since effective documentation has a mix of function references, high-level context, examples, and other content. Quartodoc is a new documentation system that automatically generates Python function references within Quarto websites. This talk will discuss pkgdown‚Äôs success in the R ecosystem and how those wins can be replicated in Python with quartodoc examples. Listeners will walk away knowing more about what makes documentation delightful (or painful), when to use quartodoc, and how to use this tool to make docs for a Python package.\nView the repository\n\n\n\nI had so much fun as posit::conf(2023)! The community here is ELECTRIC. Everyone is so engaged and interested and genuine (and nerdy). It feels like a show and tell with all your best friends. I had the pleasure of talking about my documentation journey with Python packages, and how the project quartodoc felt just right."
  },
  {
    "objectID": "talks/berlinbuzzwords2021/index.html",
    "href": "talks/berlinbuzzwords2021/index.html",
    "title": "Explaining model explainability",
    "section": "",
    "text": "Machine learning doesn‚Äôt have the same objectives as its users. While models look to optimize a function using the given data, humans look to gain insight into their problems. At best, these two objectives align; at worst, machine learning models make the front page of the news for unintended, but astonishing bias. Model explainability algorithms allow data scientists to understand not only what the model outcome is, but why it is being made. This talk will explain what model explainability is, who should care, and show participants how/when to use multiple types of explainability algorithms.\nThis session shows the usefulness of a variety of algorithms, but also discusses the limitations. Told from a data scientist‚Äôs point of view, this session provides a use case scenario exposing unintended bias using healthcare data. The audience will learn: the basics of model explainability, why this is a relevant issue, how model explainability offers insight into unintended bias, and know how to deploy explainability algorithms in Python with alibi, the open-source library from Seldon.\nWatch the recording"
  },
  {
    "objectID": "talks/berlinbuzzwords2021/index.html#abstract",
    "href": "talks/berlinbuzzwords2021/index.html#abstract",
    "title": "Explaining model explainability",
    "section": "",
    "text": "Machine learning doesn‚Äôt have the same objectives as its users. While models look to optimize a function using the given data, humans look to gain insight into their problems. At best, these two objectives align; at worst, machine learning models make the front page of the news for unintended, but astonishing bias. Model explainability algorithms allow data scientists to understand not only what the model outcome is, but why it is being made. This talk will explain what model explainability is, who should care, and show participants how/when to use multiple types of explainability algorithms.\nThis session shows the usefulness of a variety of algorithms, but also discusses the limitations. Told from a data scientist‚Äôs point of view, this session provides a use case scenario exposing unintended bias using healthcare data. The audience will learn: the basics of model explainability, why this is a relevant issue, how model explainability offers insight into unintended bias, and know how to deploy explainability algorithms in Python with alibi, the open-source library from Seldon.\nWatch the recording"
  },
  {
    "objectID": "talks/pydatanyc2022/index.html",
    "href": "talks/pydatanyc2022/index.html",
    "title": "Holistic MLOps for better science",
    "section": "",
    "text": "Machine learning operations (MLOps) are often synonymous with large and complex applications, but many MLOps practices help practitioners build better models, regardless of the size. This talk shares best practices for operationalizing a model and practical examples using the open-source MLOps framework vetiver to version, share, deploy, and monitor models.\nView the repository || Watch the recording\n\n\n\nPyData always hosts such a great conference, and PyData NYC 2022 was no exception! I met so many cool people, learned a lot of lore about pandas from other talks, and ate some delicious NY-style pizza. This talk is a good reference for explaining why the vetiver framework emphasizes pinning a model and reading it into a Dockerfile, rather than putting the model binary into the Docker container. In short, this is due to the difficulty of updating a model later and doing ad hoc analysis."
  },
  {
    "objectID": "talks/pydatanyc2022/index.html#abstract",
    "href": "talks/pydatanyc2022/index.html#abstract",
    "title": "Holistic MLOps for better science",
    "section": "",
    "text": "Machine learning operations (MLOps) are often synonymous with large and complex applications, but many MLOps practices help practitioners build better models, regardless of the size. This talk shares best practices for operationalizing a model and practical examples using the open-source MLOps framework vetiver to version, share, deploy, and monitor models.\nView the repository || Watch the recording\n\n\n\nPyData always hosts such a great conference, and PyData NYC 2022 was no exception! I met so many cool people, learned a lot of lore about pandas from other talks, and ate some delicious NY-style pizza. This talk is a good reference for explaining why the vetiver framework emphasizes pinning a model and reading it into a Dockerfile, rather than putting the model binary into the Docker container. In short, this is due to the difficulty of updating a model later and doing ad hoc analysis."
  },
  {
    "objectID": "toast.html",
    "href": "toast.html",
    "title": "Toast",
    "section": "",
    "text": "Toast is my dog! He gets a special place on my blog because 1) he is very cute and 2) he deserves partial credit for being my devoted coding rubber duck for many years. He knows over 30 tricks and has competed in barn hunt and agility."
  },
  {
    "objectID": "posts/quarto-blog-positron/index.html",
    "href": "posts/quarto-blog-positron/index.html",
    "title": "Building a portfolio website in Positron",
    "section": "",
    "text": "Note\n\n\n\nThis blog post was adapted from a workshop I gave for Florida Polytechnic University‚Äôs Data Science Club. I have a different post if you‚Äôre interested in an IDE-agnostic workflow for building Quarto websites.\nBefore we get started, you‚Äôll need two things:"
  },
  {
    "objectID": "posts/quarto-blog-positron/index.html#what-is-quarto",
    "href": "posts/quarto-blog-positron/index.html#what-is-quarto",
    "title": "Building a portfolio website in Positron",
    "section": "What is Quarto?",
    "text": "What is Quarto?\nQuarto is a next-generation publishing system that makes it easy to create beautiful documents, websites, and presentations. Combine narrative text with executable code to create reports, blogs, and presentations. Your code output appears right alongside your explanations, making your work reproducible and easy to understand."
  },
  {
    "objectID": "posts/quarto-blog-positron/index.html#what-is-a-quarto-markdown-file",
    "href": "posts/quarto-blog-positron/index.html#what-is-a-quarto-markdown-file",
    "title": "Building a portfolio website in Positron",
    "section": "What is a Quarto markdown file?",
    "text": "What is a Quarto markdown file?\nQuarto Markdown (.qmd) files are the core of your website. They combine plain text and markdown for writing narrative content with simple formatting (headings, lists, links, etc.), YAML frontmatter at the top for metadata like title, date, and formatting options, and code chunks that can either be displayed statically or executed when you render the site.\nWhen you render your website, Quarto will execute the code and display both the code and its output on your website. This is perfect for data science portfolios where you want to showcase both your code and its results. Check out the Quarto gallery for inspiration on what‚Äôs possible."
  },
  {
    "objectID": "posts/quarto-blog-positron/index.html#build-your-website-locally",
    "href": "posts/quarto-blog-positron/index.html#build-your-website-locally",
    "title": "Building a portfolio website in Positron",
    "section": "Build your website locally",
    "text": "Build your website locally\n\nCreate new folder from template\nLet‚Äôs start by creating a dedicated folder for your website project. This keeps everything organized and self-contained. All your blog posts, images, and configuration files will live in one place. In Positron, we‚Äôll use the built-in project creation tools to set up this folder structure properly.\n\n\n\nChoose empty project\nWhen Positron asks what type of project to create, select ‚ÄúEmpty Project.‚Äù Starting with an empty project gives us a clean slate to build our blog. This also will make Positron open your new folder automatically.\n\n\n\nName your folder\nNow comes an important naming decision. If you plan to host your site on GitHub Pages, name this folder GITHUB-USERNAME.github.io, replacing GITHUB-USERNAME with your actual GitHub username.\nFor example, my GitHub username is isabelizimm, so I‚Äôd the folder isabelizimm.github.io. This specific naming convention tells GitHub to automatically host your site at https://janedoe.github.io. If you‚Äôre using Quarto Pub instead, you can choose any name you like.\n\n\n\nView new folder\nGreat! Positron has created your project folder and opened it as your active workspace. You should see a clean, empty project structure in the file explorer on the left side of the window. You‚Äôll know you‚Äôre in the right place because you‚Äôll see the name of your folder in the top left corner.\nThis folder is now the root directory of your future website. Everything we create from here will be relative to this location.\n\n\n\nClick on ‚ÄúSearch bar at the top‚Äù\nNow we‚Äôll use Positron‚Äôs command palette. This is a super-powered way to navigate through many of the IDE features. Click on the search bar at the top of the window. From here, select Show and Run Commands to open the full command palette where we can find Quarto-specific commands. You can also open the Command palette using the keyboard shortcut (Ctrl+Shift+P on Windows or Cmd+Shift+P on Mac).\n\n\n\nRun Quarto: Create Project\nIn the command palette that appears, type Quarto: Create Project and select this option from the list. You don‚Äôt need to type the whole thing‚ÄîPositron will filter the commands as you type.\nThis command launches Quarto‚Äôs project creation wizard, which will walk us through setting up a proper Quarto blog structure with all the necessary configuration files.\n\n\n\nCreate Blog Project\nQuarto gives you two main options here: Website or Blog. You can always switch this later, a blog can become a website or vice versa. To help you choose:\n\nBlog: Perfect for a portfolio site where you‚Äôll be publishing dated posts, articles, or project write-ups. When I was a student, I would create a new blog post each class to take notes! It was a really nice way to organize and display information, and I had a nice course portfolio at the end of each semester.\nWebsite: Better for static content like documentation or a simple landing page.\n\nFor a portfolio showcasing your data science projects, a blog is usually the better choice.\n\n\n\nPopulate website in your GITHUB-USERNAME.github.io directory\nWhen prompted to select a directory, navigate to and select the folder you created earlier‚Äîthe one named GITHUB-USERNAME.github.io (or whatever you named it if not using GitHub Pages).\nQuarto will populate this folder with a complete blog template, including sample posts, configuration files, and styling. This gives you a fully functional blog right out of the box that you can customize however you‚Äôd like.\n\n\n\nSee your beautiful blog!\nCongratulations! You now have a fully functional blog structure. Take a moment to explore the files Quarto created:\n\n_quarto.yml is the core configuration file of your website. This is where you‚Äôll customize your site‚Äôs title, navigation menu, theme, and other global settings.\nFiles ending in *.qmd are Quarto Markdown files‚Äîthese are your actual blog posts and pages. Each .qmd file becomes a page on your website.\nThe posts/ directory contains your blog posts, each in its own folder.\nindex.qmd is your homepage.\n\nTo preview your site, open any *.qmd file and click the Preview button in the top right. Positron will render your site and open it in a browser window.\n\n\n\n\n\n\nTip\n\n\n\nIn the _quarto.yml file, there is a line that says something like site-url: https://your-website-url.example.com. In order to publish properly, you‚Äôll need to delete this line. Failure to do so may cause errors at time of deployment!"
  },
  {
    "objectID": "posts/quarto-blog-positron/index.html#deploy-your-site",
    "href": "posts/quarto-blog-positron/index.html#deploy-your-site",
    "title": "Building a portfolio website in Positron",
    "section": "Deploy your site",
    "text": "Deploy your site\nNow that you have a blog locally, it‚Äôs time to share it with the world! The deployment process differs slightly depending on whether you‚Äôre using GitHub Pages or Quarto Pub.\n\nGitHub PagesQuarto Pub\n\n\n\nCreate repository for your blog\nFirst, head to GitHub and create a new repository. This is crucial: the repository must be named exactly your-username.github.io, where your-username matches your GitHub username exactly. This special naming convention tells GitHub to treat this as a GitHub Pages site and host it at that URL. Make sure to create it as a public repository. GitHub Pages only works with public repos on the free tier.\n\n\n\nCopy command from GitHub\nAfter creating the repository, GitHub will show you a setup page with several commands. Look for the section titled ‚Äú‚Ä¶or push an existing repository from the command line‚Äù and copy those commands.\nThese commands will connect your local blog folder to your GitHub repository and push your files up to GitHub.\n\n\n\nPaste command in terminal\nBack in Positron, make sure you‚Äôre in the TERMINAL tab at the bottom of the window (not the CONSOLE tab, which is for running R or Python code).\nPaste the commands you copied from GitHub and press Enter. This will initialize a git repository in your folder, connect it to GitHub, and push your files. You may be prompted to log into GitHub, follow the authentication flow if needed.\n\n\n\nPublish blog to GitHub Pages\nIt is time to publish your blog! In the terminal, run:\nquarto publish gh-pages\nThis command does several things automatically:\n\nRenders your entire website\nCreates a special gh-pages branch in your repository\nPushes the rendered HTML files to that branch\nConfigures GitHub Pages to serve your site\n\nQuarto will ask for confirmation‚Äîtype ‚ÄòY‚Äô and press Enter. In a few moments, your site will be live on the internet.\n\n\n\nConfigure GitHub Pages settings\nAlmost there! Go to your GitHub repository in your browser, click on Settings ‚Üí Pages (in the left sidebar).\nUnder ‚ÄúSource,‚Äù select ‚ÄúDeploy from a branch‚Äù and choose the gh-pages branch from the dropdown. Leave the folder as / (root).\nClick Save. GitHub will now automatically serve your website whenever you update it. Your site will be live at https://your-username.github.io within a few minutes!\n\n\n\nSend your source code to GitHub\nThere‚Äôs one final step: pushing your source files (the .qmd files) to GitHub‚Äôs main branch. This is separate from the gh-pages branch that contains your rendered HTML.\nIn Positron‚Äôs Source Control panel (the icon that looks like a funky Y with circles), you‚Äôll see all your files listed. Click the + icon to stage all changes, add a commit message like ‚ÄúInitial blog setup,‚Äù and click the checkmark.\nThen click ‚ÄúSync changes‚Äù to send your source files to GitHub. Now both your source code and your rendered website are safely stored on GitHub!\n\n\n\n\n\nPublish blog to Quarto Pub\nQuarto Pub is slightly simpler than GitHub Pages, but has the drawback of not hosting source code. In the terminal, run:\nquarto publish quarto-pub\nThe first time you do this, Quarto will open a browser window asking you to authorize the connection to Quarto Pub. Follow the prompts to log in and grant permission.\nYou‚Äôll then be asked to choose a name for your site‚Äîthis will be part of your URL (like yourname.quarto.pub). Choose something professional and memorable, like portfolio.\nThat‚Äôs it! Your site will be live at https://yourname.quarto.pub/portfolio in seconds. Every time you want to update your site, just run quarto publish quarto-pub again."
  },
  {
    "objectID": "posts/quarto-blog-positron/index.html#whats-next",
    "href": "posts/quarto-blog-positron/index.html#whats-next",
    "title": "Building a portfolio website in Positron",
    "section": "What‚Äôs next?",
    "text": "What‚Äôs next?\nNow that your blog is up and running, here are some ideas for next steps:\n\nEdit _quarto.yml to change website titles, themes, and layout\nCreate blog posts from projects you‚Äôve made\nInclude CSS files to customize the design. Tools like Claude and ChatGPT are really great at writing CSS, so you don‚Äôt need to be an expert to make the site your own.\n\nRemember, every time you make changes:\n\nPreview locally to make sure everything looks good\nRun quarto publish gh-pages (or quarto publish quarto-pub) to update your live site\nCommit and push your source files to GitHub (if using GitHub)"
  }
]
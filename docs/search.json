[
  {
    "objectID": "posts/pet-image-detection/image-explainability.html",
    "href": "posts/pet-image-detection/image-explainability.html",
    "title": "Cat detection with explainers",
    "section": "",
    "text": "The growing availability of big data has increased the benefits of using complex models, forcing data scientists to choose between accuracy and interpretability of a model’s output. Adding explainability methods to models that are not easily understood helps:\n\nEnsure transparency algorithmic decision-making\nIdentify potential bias in the training data\nGive a deeper understanding of the process being modeled\n\nThese methods are able to give insight into why your model generates specific outputs; explainability algorithms are especially useful in highly regulated industries (ie, pinpoint the attributes that caused someone to be denied/approved a home loan). We’ll demonstrate an anchor explainer in this notebook to better understand why a generic image detection model is creating certain outputs using the open-source library alibi.\n\nimport os\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3, decode_predictions\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom alibi.explainers import AnchorImage\n\nTo begin, we load and format the photo into a machine readable array.\n\ndef image_pipeline(image_name):  \n\n    #Format custom picture\n    resized_image = load_img(image_name, target_size=(299, 299))\n    numpy_image = img_to_array(resized_image)\n    image_batch = np.expand_dims(numpy_image, axis=0)\n    \n    model = InceptionV3(weights='imagenet')\n    preds = model.predict(image_batch)\n    \n    #Display results\n    label = decode_predictions(preds, top=3)\n    plt.title(label[0])\n    plt.imshow(resized_image)\n\n\nimage_name = 'cat.jpg'\nimage_pipeline(image_name)\n\n\n\n\nThe predict function returns the predictions via Softmax, which mean that the prediction can be translated as the probability that the image falls into one of the categories given in training. Even though we see this as a cat, the model gives a 100% probability that the image shows a pitcher. There are a few things to note here.\n\nThis is a photo I took of my own cat, so I can confidently know that the model has never seen the picture before. To a human eye, it is (hopefully) pretty obvious that the image is of a cat. However, oftentimes training data, especially images, does not accurately reflect photos that are taken by you or I. For example, the training photos may all have centered images in good lighting with nothing in the background, which is unrealistic for everyday images.\nThis is a Softmax output. The model only gives us the names of its best guesses, not how strong the prediction is. This is a subtle differentiation. The model believes that pitcher is definitely the best guess, but it could only be 1% certain in this prediction. To solve this, we could add new outcome metrics, or tweak the model.\n\nBefore we do any of that though, it could be useful for us to better understand how the model is making this prediction. We’ll use something called an explainer to dive deeper."
  },
  {
    "objectID": "posts/pet-image-detection/image-explainability.html#explainability",
    "href": "posts/pet-image-detection/image-explainability.html#explainability",
    "title": "Cat detection with explainers",
    "section": "Explainability",
    "text": "Explainability\nWe will use a local version of the model to build an anchor explainer. This will show us what parts of the photo the model used in order to give the “pitcher” prediction.\n\ndef explainability_pipeline(image_name): \n    \n    # format custom picture\n    resized_image = load_img(image_name, target_size=(299, 299))\n    numpy_image = img_to_array(resized_image)\n    image_batch = np.expand_dims(numpy_image, axis=0)\n    \n    # set hyperparameters\n    image_shape = (299, 299, 3)\n    segmentation_fn = 'slic'\n    kwargs = {'n_segments': 15, 'compactness': 20, 'sigma': .5}\n\n    # load model\n    model = InceptionV3(weights='imagenet')\n    \n    # explainer, from alibi library\n    explainer = AnchorImage(model, image_shape, segmentation_fn=segmentation_fn, \n                            segmentation_kwargs=kwargs, images_background=None)\n    \n    cat_image = image_batch[0]\n    explanation = explainer.explain(cat_image, threshold=.95, \\\n                                    p_sample=.5, tau=0.25)\n\n    plt.imshow(explanation.anchor)\n\n\nexplainability_pipeline(image_name)\n\nskimage.measure.label's indexing starts from 0. In future version it will start from 1. To disable this warning, explicitely set the `start_label` parameter to 1.\n\n\n\n\n\nIt looks as though the model uses primarily the background to create this prediction, so it comes as no surprise that the classification is wildly incorrect. With this information in mind, data scientists may decide to go back and create a more robust exploration of the data and model. It may be the case that the training data of cats has only solid backgrounds, different lighting, different color/hair length cat, or some other feature that caused this image to not be identified correctly.\nExplainers will not fix the model itself. However, they are useful tools for data scientists to build well-architected models by exposing bias in training data and giving transparency to black-box models."
  },
  {
    "objectID": "posts/pet-image-detection/image-explainability.html#try-your-own-pets",
    "href": "posts/pet-image-detection/image-explainability.html#try-your-own-pets",
    "title": "Cat detection with explainers",
    "section": "Try your own pets!",
    "text": "Try your own pets!\nEither download or git clone this demo repo to your local machine. Import your own photo to the data folder. Change your-image-name in the final cell (see below for example) to match your image’s name, and press run!\n\ncustom_image_name = '../data/your-image-name.jpg'\nimage_pipeline(custom_image_name)\nexplainability_pipeline(custom_image_name)"
  },
  {
    "objectID": "posts/gha-making-gha/gha.html",
    "href": "posts/gha-making-gha/gha.html",
    "title": "Github Actions all the way down",
    "section": "",
    "text": "See it in (Github) action\nGitHub Action that pushes to a different repo.\nGitHub Actions in receiving repo that runs once a push has been made.\n\n\nSet up\nYou will need two different repos, a sending repo and receiving repo. In my case, the sending repo generates a cookiecutter template for a Python package, and then the receiving repo is the output package. You don’t have to be familiar with cookiecutter templates to understand this gist, but they’re worth a quick click if organization brings you joy.\nBefore running your chain reaction of actions, a few pieces need to be set up. The receiving repo cannot be empty, so I just initialized it with an empty README.md. The next part is probably the most difficult if you’re unfamiliar, which is generating an SSH key, click here for instructions. Once you’ve gone into the terminal and generated the key, add the PUBLIC KEY to your SENDING repo and the PRIVATE KEY to your RECEIVING repo.\n\n\nReady, ACTION!\nThe GitHub Action that sets off this package creation is below, or you can see the action in the repo, for context. Note: there are difference between the code below and the code in the repo, these changes were made for clarity and should not impact performance.\n# Creates cookie cutter and pushes to new repo\nname: Generate Cookiecutter\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n  workflow_dispatch:\n  \njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - uses: actions/setup-python@v2\n      - name: Install requirements\n        run: pip install cookiecutter\n      - name: Make cookiecutter with default inputs\n        run: cookiecutter --no-input .\n      - name: Send to receiving repo\n        uses: peaceiris/actions-gh-pages@v3\n        with:\n          deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }}\n          external_repository: isabelizimm/making-cookies\n          publish_branch: dev-branch\n          publish_dir: ./my_new_package\n          exclude_assets: ''\nReally, the most important part here is the Send to receiving repo step. You see here that your sender deploy key is being utilized and the external repository and branch is specified. This Action will normally exclude certain files (in particular, we need .github/ from the template) on push, which is we have to manually set exclude_assets: ' ' so all files are being pushed.\nOnce this action is running successfully, you will see a new branch, dev-branch, on the receiving repo, making-cookies. Congratulations! The hard part is finished.\nThe second set of actions is less magical. This template package has built-in GitHub Actions to jump-start running tests and building docs. This second set of actions runs on push to any branch named “dev-*”, so they automatically start after the initial push to the branch from the sending repo.\nHopefully this helps (or at least gives a starting point) others who are also exploring the capabilites of CI! Feel free to contact me if you have any suggestions or struggles. :)\ncheers, iz"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "isabel zimmerman",
    "section": "",
    "text": "Isabel Zimmerman is a software engineer at RStudio, PBC, where she builds Python-based MLOps frameworks. When not thinking about machine learning systems, she enjoys gardening and competing in agility competitions with her dog, Toast."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "isabel zimmerman",
    "section": "Education",
    "text": "Education\nFlorida Polytechnic University | Lakeland, FL\nMS in Computer Science | Aug 2021 - May 2023\nFlorida Polytechnic University | Lakeland, FL\nBS in Data Science | Aug 2017 - May 2021"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "blog",
    "section": "",
    "text": "ci\n\n\ngha\n\n\n\n\nUsing Github Actions to create new repos that also run their own Github Actions.\n\n\n\n\n\n\nMar 30, 2022\n\n\nIsabel Zimmerman\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nexplainers\n\n\nML\n\n\n\n\nAnchor explainers on images to better understand model decision-making.\n\n\n\n\n\n\nMar 20, 2022\n\n\nisabel zimmerman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/data-viz-1/mini-project.html",
    "href": "posts/data-viz-1/mini-project.html",
    "title": "CAP5735: Data Visualization - Mini-Project 1",
    "section": "",
    "text": "The Bechdel Test is a famous test for content to see if there are at least two characters who are women, that talk to each other, and the conversation is on a topic other a male character. While the bar for this criteria truly seems to be on the floor, a vast amount of movies still do not pass the test. Let’s discover more with a dataset offered by Tidy Tuesday."
  },
  {
    "objectID": "posts/data-viz-1/mini-project.html#are-we-better-at-writing-movies-that-pass-the-bechdel-test-now",
    "href": "posts/data-viz-1/mini-project.html#are-we-better-at-writing-movies-that-pass-the-bechdel-test-now",
    "title": "CAP5735: Data Visualization - Mini-Project 1",
    "section": "Are we better at writing movies that pass the Bechdel test now?",
    "text": "Are we better at writing movies that pass the Bechdel test now?\n\nraw %>% \n  select(year, binary) %>% \n  count(year, binary) %>%\n  ggplot(mapping = aes(x = year, y = n, color = binary)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Year\", y = \"Number of films\",\n       color = \"Test results\",\n       title = \"Results of Bechdel test, per year\",\n       caption = \"Source: Tidy Tuesday\")\n\n\n\n\nThis visualization seeks to show if modern movies are passing the Bechdel test more than in the past. Realistically, the biggest takeaway I get here is that there are a lot more movies in this dataset after the year 2000. It is hard to intuitively understand the relationship between passing and failing movies. Instead, let’s translate these into percentages per year of passing/failing.\n\ntotal_movies_year <- raw %>%\n  select(year) %>% \n  count(year) %>% \n  rename(total_movies=n)\n\npass_per_year <- raw %>%\n  select(binary, year) %>% \n  filter(binary == \"PASS\") %>% \n  group_by(year, binary) %>% \n  count(binary) %>% \n  rename(total_pass=n)\n\nby_year <- left_join(total_movies_year, pass_per_year) \n\n\nby_year %>% \n  select(-binary) %>% \n  replace(is.na(.), 0) %>% \n  filter(year > 1972) %>% \n  mutate(percent_passing = total_pass / total_movies) %>% \n  ggplot(mapping = aes(x = year, y = percent_passing)) +\n  geom_point(mapping = aes(size = total_movies), color = \"grey\") + \n  geom_line(color = \"grey\") +\n  geom_smooth(method = \"loess\",\n              se = FALSE,\n              size = 0.5,\n              color = \"red\") +\n  scale_x_log10() +\n  theme_minimal() +\n  coord_cartesian(ylim = c(0,1)) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Year\", y = \"Percent passing\",\n       title = \"We haven't gotten much *that* better at the Bechdel test\",\n       subtitle = \"Percent of films passing Bechdel test, per year\",\n       size = \"Number of movies released\",\n       caption = \"Source: Tidy Tuesday\")\n\n\n\n\nThis plot gives a clearer idea of how films have done on the Bechdel test across the years. The size of the dots signifies that there are more movies being released now than decades ago. I chose to make the data points grey and line of best fit red for high contrast. I added labels to all axes, to give users context for each data point. I also removed the year 1970, which only had 1 movie recorded. Finally, the autofit plot zoomed in on the data points, making it fill the space. However, I explicitly set the y-axis from 0 to 1, to better contextualize the fact that generally less than 50% of the films were passing."
  },
  {
    "objectID": "posts/data-viz-1/mini-project.html#is-a-bechdel-passing-test-movie-a-good-business-decision",
    "href": "posts/data-viz-1/mini-project.html#is-a-bechdel-passing-test-movie-a-good-business-decision",
    "title": "CAP5735: Data Visualization - Mini-Project 1",
    "section": "Is a Bechdel-passing test movie a good business decision?",
    "text": "Is a Bechdel-passing test movie a good business decision?\n\nraw %>% \n  select(intgross, binary, year) %>%\n  filter(year >= 2000, intgross < 1000000000) %>% \n  drop_na() %>% \n  ggplot(mapping = aes(y = as.numeric(intgross), factor(binary))) +\n  geom_violin() +\n  scale_y_continuous(labels = scales::dollar) +\n  theme_minimal() +\n  labs(x = \" \", title = \"Profit for Bechdel passing/non-passing films\",\n       subtitle = \"Based off international gross profit\",\n       y = \"Profit\",\n       size = \"Total number of movies released\",\n       caption = \"Source: Tidy Tuesday\")\n\n\n\n\nThis plot seeks to see if movies that pass the Bechdel test make more profit (in a very binary view of what “good business decision” means). We can see here that the majority of movies, that make less than $250,000,000, have similar profit structures, regardless of test results. However, more high-profit films fail the Bechdel test. You could extend this story by breaking up high-profit and low-profit films, and see the distribution. I chose violin plots, since they give users a clearer picture of multi-modal distributions than box plots, as seen above."
  }
]